import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from collections import deque
import random
import os
import sys
import datetime
import copy

# æ’²å…‹ç‰Œçš„èŠ±è‰²å’Œæ•¸å­—
SUITS = ['â™ ', 'â™¥', 'â™¦', 'â™£']
RANKS = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']

class Card:
    def __init__(self, suit, rank):
        self.suit = suit
        self.rank = rank
        self.is_face_up = False
    
    def __str__(self):
        if self.is_face_up:
            return f"{self.suit}{self.rank}"
        return "ğŸ‚ "  # ç‰ŒèƒŒ

class SolitaireEnv:
    def __init__(self):
        """åˆå§‹åŒ–æ¥é¾ç’°å¢ƒ"""
        tableau_size = 7 * 13 * 3     # 7åˆ—ï¼Œæ¯åˆ—æœ€å¤š13å¼µç‰Œï¼Œæ¯å¼µç‰Œ3å€‹ç‰¹å¾µ = 273
        foundation_size = 4 * 3       # 4å€‹åŸºç¤å †ï¼Œæ¯å †é ‚éƒ¨ç‰Œ3å€‹ç‰¹å¾µ = 12
        waste_size = 1 * 3           # å»¢ç‰Œå †é ‚éƒ¨1å¼µç‰Œï¼Œ3å€‹ç‰¹å¾µ = 3
        stock_size = 1               # è‚¡ç¥¨å †å¤§å°ä¿¡æ¯ = 1
        hidden_cards_size = 52 * 3   # æ‰€æœ‰éš±è—ç‰Œçš„ä¿¡æ¯ = 156
        
        self.state_size = 445  # æ›´æ–°ç‚ºå¯¦éš›è¨ˆç®—å‡ºçš„å¤§å°
        
        # åˆå§‹åŒ–å…¶ä»–å±¬æ€§
        self.tableau = [[] for _ in range(7)]
        self.foundation = [[] for _ in range(4)]
        self.stock = []
        self.waste = []
        self.move_history = []
        self.hidden_cards = {}
        self.moves_without_progress = 0
        self.history = []

    def _can_move_to_foundation(self, card, foundation_index):
        """æª¢æŸ¥ç‰Œæ˜¯å¦å¯ä»¥ç§»å‹•åˆ°æŒ‡å®šçš„foundationå †"""
        dest_pile = self.foundation[foundation_index]
        
        # å¦‚æœfoundationç‚ºç©ºï¼Œåªèƒ½æ”¾A
        if not dest_pile:
            return card.rank == 'A'
        
        # æª¢æŸ¥æ˜¯å¦åŒèŠ±è‰²ä¸”é †åºæ­£ç¢º
        top_card = dest_pile[-1]
        return (card.suit == top_card.suit and 
                RANKS.index(card.rank) == RANKS.index(top_card.rank) + 1)

    def save_decision_point(self):
        """ä¿å­˜ç•¶å‰ç‹€æ…‹ä½œç‚ºæ±ºç­–é»"""
        current_state = GameState()
        current_state.tableau = copy.deepcopy(self.tableau)
        current_state.foundation = copy.deepcopy(self.foundation)
        current_state.stock = copy.deepcopy(self.stock)
        current_state.waste = copy.deepcopy(self.waste)
        current_state.known_cards = copy.deepcopy(self.known_cards)
        self.decision_points.append({
            'state': current_state,
            'moves_tried': set(),
            'position': len(self.state_history)
        })

    def backtrack(self):
        """å›æº¯åˆ°ä¸Šä¸€å€‹æ±ºç­–é»"""
        if not self.decision_points:
            return None
            
        last_point = self.decision_points[-1]
        valid_moves = self.get_valid_moves()
        untried_moves = [move for move in valid_moves 
                        if move not in last_point['moves_tried']]
        
        if not untried_moves:
            self.decision_points.pop()
            return self.backtrack()
            
        # æ¢å¾©åˆ°æ±ºç­–é»çš„ç‹€æ…‹
        state = last_point['state']
        self.tableau = copy.deepcopy(state.tableau)
        self.foundation = copy.deepcopy(state.foundation)
        self.stock = copy.deepcopy(state.stock)
        self.waste = copy.deepcopy(state.waste)
        self.known_cards = copy.deepcopy(state.known_cards)
        
        # é¸æ“‡ä¸€å€‹æœªå˜—è©¦çš„ç§»å‹•
        next_move = untried_moves[0]
        last_point['moves_tried'].add(next_move)
        
        return next_move

    def remember_card(self, position, card):
        """è¨˜éŒ„æ–°ç™¼ç¾çš„ç‰Œ"""
        self.known_cards[position] = card
        
    def get_known_cards_info(self):
        """ç²å–å·²çŸ¥ç‰Œçš„ä¿¡æ¯"""
        return self.known_cards

    def is_deadlock(self):
        """æª¢æŸ¥æ˜¯å¦é™·å…¥æ­»å±€"""
        # å¯¦ç¾æ­»å±€æª¢æ¸¬é‚è¼¯
        # ä¾‹å¦‚ï¼šé€£çºŒå¤šæ¬¡æ²’æœ‰æ–°çš„æœ‰æ•ˆç§»å‹•
        # æˆ–è€…ç™¼ç¾æŸäº›é—œéµç‰Œè¢«é˜»å¡ç­‰æƒ…æ³
        pass
    
    # åœ¨ SolitaireEnv çš„ reset æ–¹æ³•ä¸­
    def reset(self, custom_deck=None):
        """é‡ç½®éŠæˆ²ç‹€æ…‹"""
        # åˆå§‹åŒ–å„å€‹ç‰Œå †
        self.tableau = [[] for _ in range(7)]
        self.foundation = [[] for _ in range(4)]
        self.stock = []
        self.waste = []
        self.move_history = []
        self.hidden_cards = {}
        self.moves_without_progress = 0
        
        if custom_deck:
            # ä½¿ç”¨è‡ªå®šç¾©ç‰Œçµ„
            deck = []
            for suit, rank in custom_deck:
                card = Card(suit, rank)
                deck.append(card)
            
            # æŒ‰åˆ—ç™¼ç‰Œï¼ˆå¾å·¦åˆ°å³ï¼Œæ¯åˆ—å¾ä¸Šåˆ°ä¸‹ï¼‰
            card_index = 0
            for col in range(7):  # 7åˆ—
                for row in range(col + 1):  # æ¯åˆ—çš„ç‰Œæ•¸
                    if card_index >= len(deck):
                        print(f"Warning: Not enough cards in custom deck")
                        break
                    card = deck[card_index]
                    # åªæœ‰æœ€å¾Œä¸€å¼µç‰Œæ­£é¢æœä¸Š
                    card.is_face_up = (row == col)
                    self.tableau[col].append(card)
                    if not card.is_face_up:
                        self.hidden_cards[(col, row)] = (card.suit, card.rank)
                    card_index += 1
            
            # å‰©é¤˜çš„ç‰Œæ”¾å…¥stock
            self.stock = deck[card_index:]
            
            # é©—è­‰ç™¼ç‰Œçµæœ
            print("\nVerifying tableau distribution:")
            for i, pile in enumerate(self.tableau):
                print(f"Column {i} ({len(pile)} cards): ", end="")
                for card in pile:
                    if card.is_face_up:
                        print(f"{card.suit}{card.rank}* ", end="")
                    else:
                        print(f"{card.suit}{card.rank} ", end="")
                print()
            
            print(f"\nStock ({len(self.stock)} cards): ", end="")
            for card in self.stock:
                print(f"{card.suit}{card.rank} ", end="")
            print("\n")
            
        else:
            # å‰µå»ºä¸¦æ´—ç‰Œæ¨™æº–52å¼µç‰Œ
            deck = []
            for suit in ['â™ ', 'â™¥', 'â™¦', 'â™£']:
                for rank in ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']:
                    card = Card(suit, rank)
                    deck.append(card)
            
            random.shuffle(deck)
            
            # æŒ‰åˆ—ç™¼ç‰Œ
            card_index = 0
            for col in range(7):
                for row in range(col + 1):
                    card = deck[card_index]
                    card.is_face_up = (row == col)
                    self.tableau[col].append(card)
                    if not card.is_face_up:
                        self.hidden_cards[(col, row)] = (card.suit, card.rank)
                    card_index += 1
            
            # å‰©é¤˜çš„ç‰Œæ”¾å…¥stock
            self.stock = deck[card_index:]
        
        # è¿”å›åˆå§‹ç‹€æ…‹
        initial_state = self._get_state()
        return initial_state, {}
    
    def _get_state(self):
        """ç²å–ç•¶å‰éŠæˆ²ç‹€æ…‹çš„å‘é‡è¡¨ç¤º"""
        state = []
        
        # Tableau ç‹€æ…‹ (273 = 7 piles * 13 cards * 3 features)
        for pile in self.tableau:
            pile_state = []
            for i in range(13):  # æœ€å¤š13å¼µç‰Œ
                if i < len(pile):
                    card = pile[i]
                    # æ¯å¼µç‰Œç”¨3å€‹å€¼è¡¨ç¤ºï¼šèŠ±è‰²(4ç¨®)ã€é»æ•¸(13ç¨®)ã€æ˜¯å¦æœä¸Š
                    pile_state.extend([
                        ['â™ ', 'â™¥', 'â™¦', 'â™£'].index(card.suit) / 3,
                        RANKS.index(card.rank) / 12,
                        1.0 if card.is_face_up else 0.0
                    ])
                else:
                    pile_state.extend([0, 0, 0])  # ç©ºä½ç½®
            state.extend(pile_state)
        
        # Foundation ç‹€æ…‹ (12 = 4 piles * 3 features)
        for pile in self.foundation:
            if pile:
                top_card = pile[-1]
                state.extend([
                    ['â™ ', 'â™¥', 'â™¦', 'â™£'].index(top_card.suit) / 3,
                    RANKS.index(top_card.rank) / 12,
                    len(pile) / 13
                ])
            else:
                state.extend([0, 0, 0])
        
        # Waste ç‹€æ…‹ (3 features)
        if self.waste:
            top_card = self.waste[-1]
            state.extend([
                ['â™ ', 'â™¥', 'â™¦', 'â™£'].index(top_card.suit) / 3,
                RANKS.index(top_card.rank) / 12,
                1.0
            ])
        else:
            state.extend([0, 0, 0])
        
        # Stock ç‹€æ…‹ (1 feature)
        state.append(len(self.stock) / 24)  # æ¨™æº–åŒ–
        
        # Hidden cards ä¿¡æ¯ (156 = 4 suits * 13 ranks * 3 features)
        hidden_state = np.zeros(156)
        for (pile_idx, card_idx), (suit, rank) in self.hidden_cards.items():
            base_idx = (RANKS.index(rank) * 4 + ['â™ ', 'â™¥', 'â™¦', 'â™£'].index(suit)) * 3
            hidden_state[base_idx:base_idx+3] = [
                ['â™ ', 'â™¥', 'â™¦', 'â™£'].index(suit) / 3,
                RANKS.index(rank) / 12,
                1.0
            ]
        state.extend(hidden_state)
        
        # ç¢ºä¿ç‹€æ…‹å‘é‡å¤§å°æ­£ç¢º
        state_array = np.array(state, dtype=np.float32)
        assert len(state_array) == 445, f"State size mismatch: got {len(state_array)}, expected 445"
        
        return state_array
    
    def _is_valid_move(self, card, dest_type, dest_pile):
        """æª¢æŸ¥ç§»å‹•æ˜¯å¦æœ‰æ•ˆ"""
        if dest_type == 'foundation':
            # å¦‚æœç›®æ¨™æ˜¯foundation
            if not dest_pile:  # å¦‚æœfoundationç‚ºç©º
                return card.rank == 'A'  # åªèƒ½æ”¾A
            else:
                # æª¢æŸ¥æ˜¯å¦åŒèŠ±è‰²ä¸”é †åºæ­£ç¢º
                top_card = dest_pile[-1]
                return (card.suit == top_card.suit and 
                    RANKS.index(card.rank) == RANKS.index(top_card.rank) + 1)
        
        elif dest_type == 'tableau':
            # å¦‚æœç›®æ¨™æ˜¯tableau
            if not dest_pile:  # å¦‚æœtableauç‚ºç©º
                return card.rank == 'K'  # åªèƒ½æ”¾K
            else:
                # æª¢æŸ¥é¡è‰²æ˜¯å¦ç›¸åä¸”é †åºæ­£ç¢º
                top_card = dest_pile[-1]
                return (self._is_opposite_color(card.suit, top_card.suit) and 
                    RANKS.index(card.rank) == RANKS.index(top_card.rank) - 1)
        
        return False

    def _is_opposite_color(self, suit1, suit2):
        """æª¢æŸ¥å…©å€‹èŠ±è‰²æ˜¯å¦é¡è‰²ç›¸å"""
        return ((suit1 in ['â™¥', 'â™¦'] and suit2 in ['â™ ', 'â™£']) or
                (suit1 in ['â™ ', 'â™£'] and suit2 in ['â™¥', 'â™¦']))
    
    def get_valid_moves(self):
        valid_moves = []
        
        # æª¢æŸ¥tableauåˆ°foundationçš„ç§»å‹•
        for i, source_pile in enumerate(self.tableau):
            if source_pile:
                card = source_pile[-1]
                if card.is_face_up:
                    for j in range(4):  # æª¢æŸ¥æ‰€æœ‰foundationå †
                        if self._can_move_to_foundation(card, j):
                            valid_moves.append(('tableau', i, len(source_pile)-1, 'foundation', j))

        # å¾wasteåˆ°tableauæˆ–foundationçš„ç§»å‹•
        if self.waste:
            card = self.waste[-1]
            # æª¢æŸ¥åˆ°tableauçš„ç§»å‹•
            for i in range(7):
                if self._is_valid_move(card, 'tableau', self.tableau[i]):
                    valid_moves.append(('waste', 'tableau', i))
            # æª¢æŸ¥åˆ°foundationçš„ç§»å‹•
            for i in range(4):
                if self._is_valid_move(card, 'foundation', self.foundation[i]):
                    valid_moves.append(('waste', 'foundation', i))
        
        # å¾tableauåˆ°tableauæˆ–foundationçš„ç§»å‹•
        for i in range(7):
            if not self.tableau[i]:
                continue
            for j in range(len(self.tableau[i])):
                if not self.tableau[i][j].is_face_up:
                    continue
                card = self.tableau[i][j]
                # æª¢æŸ¥åˆ°å…¶ä»–tableauåˆ—çš„ç§»å‹•
                for k in range(7):
                    if k != i and self._is_valid_move(card, 'tableau', self.tableau[k]):
                        valid_moves.append(('tableau', i, j, 'tableau', k))
                # æª¢æŸ¥åˆ°foundationçš„ç§»å‹•
                if j == len(self.tableau[i]) - 1:  # åªèƒ½ç§»å‹•é ‚ç‰Œåˆ°foundation
                    for k in range(4):
                        if self._is_valid_move(card, 'foundation', self.foundation[k]):
                            valid_moves.append(('tableau', i, j, 'foundation', k))
        
        # ç¿»stockç‰Œçš„æ“ä½œ
        if self.stock:
            valid_moves.append(('stock', 'waste'))
        elif self.waste:  # stockç©ºæ™‚å¯ä»¥å°‡wasteé‡ç½®
            valid_moves.append(('reset',))
            
        return valid_moves
    
    def _calculate_card_value(self, suit, rank):
        """è¨ˆç®—ç‰Œçš„ä¼°è¨ˆåƒ¹å€¼"""
        value = 0.0
        # åŸºç¤åƒ¹å€¼
        base_value = RANKS.index(rank) / 12.0
        
        # æª¢æŸ¥æ˜¯å¦å¯ä»¥ç›´æ¥æ”¾åˆ°foundation
        for i, pile in enumerate(self.foundation):
            if not pile and rank == 'A':
                value += 1.0
            elif pile and pile[-1].suit == suit:
                if RANKS.index(rank) == RANKS.index(pile[-1].rank) + 1:
                    value += 1.0
        
        # Aå’Œ2çš„åŸºç¤åƒ¹å€¼è¼ƒé«˜
        if rank in ['A', '2']:
            value += 0.5

        # æ ¹æ“šç•¶å‰foundationçš„æƒ…æ³èª¿æ•´åƒ¹å€¼
        for pile in self.foundation:
            if pile and pile[-1].suit == suit:
                if RANKS.index(rank) == RANKS.index(pile[-1].rank) + 1:
                    base_value *= 1.5  # å¦‚æœæ˜¯ä¸‹ä¸€å¼µéœ€è¦çš„ç‰Œ,æé«˜åƒ¹å€¼
                    break
        
        # æ ¹æ“štableauçš„æƒ…æ³èª¿æ•´
        for pile in self.tableau:
            if pile and pile[-1].is_face_up:
                top_card = pile[-1]
                if (RANKS.index(rank) == RANKS.index(top_card.rank) - 1 and
                    ((suit in ['â™¥', 'â™¦']) != (top_card.suit in ['â™¥', 'â™¦']))):
                    base_value *= 1.2  # å¦‚æœå¯ä»¥æ”¾åœ¨tableauä¸Š,ç¨å¾®æé«˜åƒ¹å€¼
                    break
        
        # æ ¹æ“šranké¡å¤–èª¿æ•´
        if rank == 'A':
            base_value *= 1.3  # Açš„åƒ¹å€¼ç•¥é«˜ï¼Œå› ç‚ºæ˜¯foundationçš„èµ·å§‹ç‰Œ
        elif rank == 'K':
            base_value *= 1.2  # Kçš„åƒ¹å€¼ä¹Ÿç•¥é«˜ï¼Œå› ç‚ºå¯ä»¥é–‹æ–°åˆ—

        rank_values = {
            'A': 1.0,
            'K': 0.85,
            'Q': 0.7,
            'J': 0.55,
            '10': 0.4,
            '9': 0.35,
            '8': 0.3,
            '7': 0.25,
            '6': 0.2,
            '5': 0.15,
            '4': 0.1,
            '3': 0.05,
            '2': 0.0
        }
        
        base_value = rank_values[rank]   
        return base_value

    def _save_state(self):
        """ä¿å­˜ç•¶å‰éŠæˆ²ç‹€æ…‹"""
        state = {
            'tableau': copy.deepcopy(self.tableau),
            'foundation': copy.deepcopy(self.foundation),
            'stock': copy.deepcopy(self.stock),
            'waste': copy.deepcopy(self.waste),
            'hidden_cards': copy.deepcopy(self.hidden_cards),
            'moves_without_progress': self.moves_without_progress,
            'move_history': copy.deepcopy(self.move_history)
        }
        self.history.append(state)

    def _restore_state(self):
        """æ¢å¾©åˆ°ä¸Šä¸€å€‹éŠæˆ²ç‹€æ…‹"""
        if not self.history:
            return False
            
        state = self.history.pop()
        self.tableau = state['tableau']
        self.foundation = state['foundation']
        self.stock = state['stock']
        self.waste = state['waste']
        self.hidden_cards = state['hidden_cards']
        self.moves_without_progress = state['moves_without_progress']
        self.move_history = state['move_history']
        return True

    def _update_hidden_cards(self):
        """æ›´æ–°è“‹ä½çš„ç‰Œçš„ä¿¡æ¯"""
        new_hidden_cards = {}
        for i, pile in enumerate(self.tableau):
            for j, card in enumerate(pile):
                if not card.is_face_up:
                    new_hidden_cards[(i, j)] = (card.suit, card.rank)
        self.hidden_cards = new_hidden_cards

    def step(self, action):
        """åŸ·è¡Œä¸€å€‹å‹•ä½œä¸¦è¿”å›æ–°ç‹€æ…‹ã€çå‹µå’Œæ˜¯å¦çµæŸ"""
        # ä¿å­˜ç•¶å‰ç‹€æ…‹
        self._save_state()
        prev_valid_moves = len(self.get_valid_moves())
        prev_foundation_cards = sum(len(pile) for pile in self.foundation)
        prev_face_up_cards = sum(1 for pile in self.tableau for card in pile if card.is_face_up)
        
        # åˆå§‹åŒ–çå‹µå’ŒçµæŸæ¨™èªŒ
        reward = 0
        done = False
        
        if action[0] == 'stock':
            # å¾stockç¿»ç‰Œåˆ°waste
            if self.stock:
                card = self.stock.pop()
                card.is_face_up = True
                self.waste.append(card)
                reward = 0.1  # å°çå‹µ
            else:
                # å¦‚æœstockç©ºäº†ï¼Œä½†wasteæœ‰ç‰Œï¼Œé‡ç½®
                if self.waste:
                    self.stock = self.waste[::-1]  # åè½‰waste
                    for card in self.stock:
                        card.is_face_up = False
                    self.waste = []
                    reward = -0.2  # å°æ‡²ç½°
                else:
                    reward = -0.5  # è¼ƒå¤§æ‡²ç½°

        if action[0] == 'tableau' and action[3] == 'foundation':
            # å¾tableauç§»åˆ°foundation
            source_pile = self.tableau[action[1]]
            if len(source_pile) > action[2]:  # ç¢ºä¿æœ‰è¶³å¤ çš„ç‰Œ
                card = source_pile[action[2]]
                if card.is_face_up:  # ç¢ºä¿ç‰Œæ˜¯æœä¸Šçš„
                    dest_pile = self.foundation[action[4]]
                    if self._is_valid_move(card, 'foundation', dest_pile):
                        # ç§»å‹•ç‰Œ
                        dest_pile.append(source_pile.pop(action[2]))
                        reward = 2.0  # æˆåŠŸç§»åˆ°foundationçš„çå‹µ
                        
                        # å¦‚æœç§»å‹•å¾Œéœ²å‡ºæ–°ç‰Œï¼Œç¿»é–‹å®ƒ
                        if source_pile and not source_pile[-1].is_face_up:
                            source_pile[-1].is_face_up = True
                            if (action[1], len(source_pile)-1) in self.hidden_cards:
                                del self.hidden_cards[(action[1], len(source_pile)-1)]
                            reward += 1.0
                    else:
                        reward = -1.0  # ç„¡æ•ˆç§»å‹•çš„æ‡²ç½°
            
        elif action[0] == 'reset':
            # é‡ç½®wasteåˆ°stock
            self.stock = list(reversed(self.waste))
            self.waste = []
            for card in self.stock:
                card.is_face_up = False
            reward = -0.2
            
        elif action[0] == 'waste':
            card = self.waste.pop()
            if action[1] == 'tableau':
                self.tableau[action[2]].append(card)
                reward = 1.0
            else:  # foundation
                self.foundation[action[2]].append(card)
                reward = 2.0
                
        elif action[0] == 'tableau':
            source_pile = self.tableau[action[1]]
            cards_to_move = source_pile[action[2]:]
            if action[3] == 'tableau':
                # æª¢æŸ¥æ˜¯å¦æ˜¯ç„¡æ„ç¾©çš„ç§»å‹•
                if self._is_meaningless_move(source_pile, action[2], self.tableau[action[4]]):
                    reward = -1.0  # åŠ å¤§æ‡²ç½°
                else:
                    reward = 0.5
                    
                self.tableau[action[4]].extend(cards_to_move)
                del source_pile[action[2]:]
            
            # ç¿»é–‹ç§»å‹•å¾Œéœ²å‡ºçš„ç‰Œ
            if source_pile and not source_pile[-1].is_face_up:
                source_pile[-1].is_face_up = True
                # æ›´æ–°hidden_cards
                if (action[1], len(source_pile)-1) in self.hidden_cards:
                    del self.hidden_cards[(action[1], len(source_pile)-1)]
                reward += 1.0
        
        # æ›´æ–°hidden_cards
        self._update_hidden_cards()
        
        # ä¿æŒæ­·å²è¨˜éŒ„åœ¨åˆç†ç¯„åœå…§
        if len(self.move_history) > 100:
            self.move_history.pop(0)

        # è¨ˆç®—é€²å±•çå‹µ
        current_valid_moves = len(self.get_valid_moves())
        moves_diff = current_valid_moves - prev_valid_moves
        
        if moves_diff > 0:
            reward += moves_diff * 0.2
        
        current_foundation_cards = sum(len(pile) for pile in self.foundation)
        current_face_up_cards = sum(1 for pile in self.tableau for card in pile if card.is_face_up)
                
        # æª¢æŸ¥æ˜¯å¦æœ‰é€£çºŒçš„foundationç‰Œ
        for i in range(4):
            if len(self.foundation[i]) >= 2:
                reward += len(self.foundation[i]) * 0.5

        # é¡å¤–çš„é€²å±•çå‹µ
        foundation_progress = current_foundation_cards - prev_foundation_cards
        face_up_progress = current_face_up_cards - prev_face_up_cards
        
        reward += foundation_progress * 2.0
        reward += face_up_progress * 0.5
        
        # æ ¹æ“šhidden_cardsçš„ä¿¡æ¯èª¿æ•´çå‹µ
        for pos, (suit, rank) in self.hidden_cards.items():
            value = self._calculate_card_value(suit, rank)
            if value > 0.7:  # å¦‚æœæ˜¯é«˜åƒ¹å€¼çš„ç‰Œé‚„è¢«è“‹è‘—
                reward -= 0.1  # è¼•å¾®æ‡²ç½°
            
        # å¢åŠ é€£çºŒæˆåŠŸçš„çå‹µ
        if foundation_progress > 0:
            reward += foundation_progress * 3.0  # å¢åŠ åŸºç¤çå‹µ
            # é‡ç½®ç„¡æ•ˆå¾ªç’°è¨ˆæ•¸å™¨
            self.moves_without_progress = 0
        
        else:
            # å¢åŠ ç„¡æ•ˆå¾ªç’°è¨ˆæ•¸å™¨
            self.moves_without_progress += 1
            
        # æ‡²ç½°ç„¡æ•ˆå¾ªç’°
        if self.moves_without_progress > 20:  # å¦‚æœé€£çºŒ20æ­¥æ²’æœ‰é€²å±•
            reward -= 0.1 * (self.moves_without_progress - 20)  # é€æ­¥å¢åŠ æ‡²ç½°
            
        if self.moves_without_progress > 50:  # å¦‚æœé€£çºŒ50æ­¥æ²’æœ‰é€²å±•
            reward -= 0.2 * (self.moves_without_progress - 50)  # é€²ä¸€æ­¥å¢åŠ æ‡²ç½°
                                    
        # æª¢æŸ¥æ˜¯å¦ç²å‹
        if all(len(pile) == 13 for pile in self.foundation):
            reward = 50
            done = True
        
        # ç²å–æ–°ç‹€æ…‹
        new_state = self._get_state()

        return new_state, reward, done

    def _is_useful_card(self, card):
        """æª¢æŸ¥ä¸€å¼µç‰Œæ˜¯å¦ç•¶å‰æœ‰ç”¨"""
        # æª¢æŸ¥æ˜¯å¦å¯ä»¥ç›´æ¥æ”¾åˆ°foundation
        for pile in self.foundation:
            if self._is_valid_move(card, 'foundation', pile):
                return True
                
        # æª¢æŸ¥æ˜¯å¦å¯ä»¥æ”¾åˆ°tableauä¸¦èƒ½å¹«åŠ©è§£é–å…¶ä»–ç‰Œ
        for i, pile in enumerate(self.tableau):
            if self._is_valid_move(card, 'tableau', pile):
                # å¦‚æœé€™å€‹ä½ç½®ä¸‹é¢æœ‰è“‹è‘—çš„ç‰Œ,å‰‡æ›´æœ‰åƒ¹å€¼
                if pile and not pile[-1].is_face_up:
                    return True
                # å¦‚æœé€™å¼µç‰Œèƒ½å¹«åŠ©è§£é–å…¶ä»–æœ‰ç”¨çš„åºåˆ—
                if self._will_unlock_useful_sequence(card, i):
                    return True
        
        return False

    def _will_unlock_useful_sequence(self, card, tableau_idx):
        """æª¢æŸ¥é€™å¼µç‰Œæ˜¯å¦èƒ½å¹«åŠ©è§£é–æœ‰ç”¨çš„åºåˆ—"""
        # ä¾‹å¦‚:å¦‚æœé€™å¼µç‰Œæ˜¯ç´…å¿ƒ5,æª¢æŸ¥æ˜¯å¦æœ‰é»‘æ¡ƒ4æˆ–æ¢…èŠ±4ç­‰å¾…å®ƒ
        target_rank = RANKS[RANKS.index(card.rank) - 1]
        target_suits = ['â™ ', 'â™£'] if card.suit in ['â™¥', 'â™¦'] else ['â™¥', 'â™¦']
        
        for pile in self.tableau:
            if pile and pile[-1].is_face_up:
                if (pile[-1].rank == target_rank and 
                    pile[-1].suit in target_suits):
                    return True
        return False
    
    def _is_meaningless_move(self, source_pile, start_idx, dest_pile):
        """æª¢æŸ¥æ˜¯å¦æ˜¯ç„¡æ„ç¾©çš„ç§»å‹•"""
        # å¦‚æœç›®æ¨™å †æ˜¯ç©ºçš„,ä½†ç§»å‹•çš„ä¸æ˜¯K,å°±æ˜¯ç„¡æ„ç¾©çš„
        if not dest_pile and source_pile[start_idx].rank != 'K':
            return True
            
        # å¦‚æœç§»å‹•å¾Œæœƒéœ²å‡ºæ–°ç‰Œ,å‰‡ä¸æ˜¯ç„¡æ„ç¾©çš„
        if start_idx > 0 and not source_pile[start_idx-1].is_face_up:
            return False
            
        # å¦‚æœé€™å€‹ç§»å‹•æœƒè®“å…¶ä»–æœ‰ç”¨çš„ç§»å‹•æˆç‚ºå¯èƒ½,å‰‡ä¸æ˜¯ç„¡æ„ç¾©çš„
        if self._enables_useful_moves(source_pile[start_idx:], dest_pile):
            return False
            
        # æª¢æŸ¥æ˜¯å¦åªæ˜¯åœ¨å…©å †ä¹‹é–“ä¾†å›ç§»å‹•
        if len(self.move_history) >= 4:
            last_moves = self.move_history[-4:]
            if self._is_cycling_moves(last_moves):
                return True
                
        return False

    def _enables_useful_moves(self, cards_to_move, dest_pile):
        """æª¢æŸ¥é€™å€‹ç§»å‹•æ˜¯å¦èƒ½ä½¿å…¶ä»–æœ‰ç”¨çš„ç§»å‹•æˆç‚ºå¯èƒ½"""
        # ä¾‹å¦‚:ç§»å‹•å¾Œèƒ½é‡‹æ”¾å‡ºä¸€å€‹ç©ºåˆ—ä¾†æ”¾K
        # æˆ–è€…èƒ½è®“æŸå¼µè¢«æ“‹ä½çš„ç‰Œå¯ä»¥ç§»å‹•åˆ°foundation
        return False  # å¯¦ç¾å…·é«”çš„æª¢æŸ¥é‚è¼¯

    def _is_cycling_moves(self, moves):
        """æª¢æŸ¥æ˜¯å¦åœ¨é€²è¡Œå¾ªç’°ç§»å‹•"""
        # æª¢æŸ¥æœ€è¿‘çš„ç§»å‹•æ˜¯å¦å½¢æˆäº†å¾ªç’°æ¨¡å¼
        if len(moves) < 4:
            return False
            
        # æª¢æŸ¥æ˜¯å¦åœ¨åŒæ¨£çš„ä½ç½®ä¹‹é–“ä¾†å›ç§»å‹•
        if (moves[0][1] == moves[2][4] and moves[0][4] == moves[2][1] and
            moves[1][1] == moves[3][4] and moves[1][4] == moves[3][1]):
            return True
            
        return False

    def _is_important_decision(self, action):
        """åˆ¤æ–·æ˜¯å¦æ˜¯é‡è¦æ±ºç­–é»"""
        if action[0] == 'tableau':
            # ç§»å‹•Kingåˆ°ç©ºåˆ—
            if (len(self.tableau[action[1]]) > 0 and 
                self.tableau[action[1]][action[2]].rank == 'K' and 
                not self.tableau[action[4]]):
                return True
            # ç§»å‹•å¤šå¼µç‰Œ
            if action[2] < len(self.tableau[action[1]]) - 1:
                return True
        elif action[0] == 'waste':
            # å¾wasteç§»å‹•åˆ°tableauçš„é‡è¦ç‰Œ
            card = self.waste[-1]
            if card.rank in ['A', 'K'] or self._is_blocking_card(card):
                return True
        return False

    def _is_blocking_card(self, card):
        """åˆ¤æ–·ä¸€å¼µç‰Œæ˜¯å¦æ˜¯é˜»å¡å…¶ä»–é‡è¦æ“ä½œçš„é—œéµç‰Œ"""
        # å¯¦ç¾æª¢æ¸¬é‚è¼¯
        pass

    def choose_backtrack_point(self):
        """æ™ºèƒ½é¸æ“‡å›æº¯é»"""
        if not self.decision_points:
            return None
            
        # è©•ä¼°æ¯å€‹æ±ºç­–é»
        best_point = None
        best_score = float('-inf')
        
        for point in reversed(self.decision_points):
            score = self._evaluate_decision_point(point)
            if score > best_score:
                best_score = score
                best_point = point
                
        return best_point

    def _evaluate_decision_point(self, point):
        """è©•ä¼°æ±ºç­–é»çš„åƒ¹å€¼"""
        state = point['state']
        score = 0
        
        # æ ¹æ“šfoundationçš„é€²åº¦
        foundation_cards = sum(len(pile) for pile in state.foundation)
        score += foundation_cards * 2
        
        # æ ¹æ“šå·²çŸ¥ç‰Œçš„ä¿¡æ¯
        known_cards = len(state.known_cards)
        score += known_cards
        
        # æ ¹æ“štableauä¸­é¢æœä¸Šçš„ç‰Œ
        face_up_cards = sum(1 for pile in state.tableau 
                        for card in pile if card.is_face_up)
        score += face_up_cards
        
        # æ ¹æ“šå‰©é¤˜çš„æœªå˜—è©¦ç§»å‹•æ•¸
        untried_moves = len(self.get_valid_moves()) - len(point['moves_tried'])
        score += untried_moves * 0.5
        
        return score

class GameState:
    def __init__(self):
        self.tableau = []
        self.foundation = []
        self.stock = []
        self.waste = []
        self.known_cards = {}  # è¨˜éŒ„å·²çŸ¥çš„ç‰Œ
        self.move_history = []  # ç§»å‹•æ­·å²
        self.decision_points = []  # é‡è¦æ±ºç­–é»
        self.score = 0
        self.moves_made = 0

    def clone(self):
        """å‰µå»ºç•¶å‰ç‹€æ…‹çš„æ·±åº¦è¤‡è£½"""
        new_state = GameState()
        new_state.tableau = copy.deepcopy(self.tableau)
        new_state.foundation = copy.deepcopy(self.foundation)
        new_state.stock = copy.deepcopy(self.stock)
        new_state.waste = copy.deepcopy(self.waste)
        new_state.known_cards = copy.deepcopy(self.known_cards)
        new_state.move_history = copy.deepcopy(self.move_history)
        new_state.score = self.score
        new_state.moves_made = self.moves_made
        return new_state

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        
        # åˆ†é›¢ç‰¹å¾µæå–
        self.tableau_net = nn.Sequential(
            nn.Linear(273, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.foundation_net = nn.Sequential(
            nn.Linear(12, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.waste_net = nn.Sequential(
            nn.Linear(3, 32),
            nn.LayerNorm(32),
            nn.ReLU()
        )
        
        self.hidden_cards_net = nn.Sequential(
            nn.Linear(156, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # åˆä½µå±¤
        self.combine = nn.Sequential(
            nn.Linear(256 + 64 + 32 + 128 + 1, 512),  # +1 for stock
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Linear(256, output_size)
        )
    
    def forward(self, x):
        # åˆ†é›¢è¼¸å…¥
        tableau = x[:, :273]
        foundation = x[:, 273:285]
        waste = x[:, 285:288]
        stock = x[:, 288:289]
        hidden_cards = x[:, 289:]
        
        # è™•ç†å„éƒ¨åˆ†
        t_out = self.tableau_net(tableau)
        f_out = self.foundation_net(foundation)
        w_out = self.waste_net(waste)
        h_out = self.hidden_cards_net(hidden_cards)
        
        # åˆä½µ
        combined = torch.cat([t_out, f_out, w_out, stock, h_out], dim=1)
        return self.combine(combined)

class SolitaireAI:
    def __init__(self, custom_deck=None):
        self.env = SolitaireEnv()
        self.custom_deck = custom_deck  
        # æ›´æ–°ç‹€æ…‹å¤§å°çš„è¨ˆç®—
        self.state_size = (
            273 +  # Tableau (7 piles * 13 cards * 3 features)
            12 +   # Foundation (4 piles * 3 features)
            3 +    # Waste (3 features)
            1 +    # Stock (1 feature)
            156    # Hidden cards (4 suits * 13 ranks * 3 features)
        )
        self.action_size = 52
        
        # åˆå§‹åŒ–ä¸»ç¶²çµ¡å’Œç›®æ¨™ç¶²çµ¡
        self.model = DQN(self.state_size, self.action_size)
        self.target_model = DQN(self.state_size, self.action_size)
        
        # æ–°çš„å­¸ç¿’åƒæ•¸
        self.learning_rate = 0.0005
        self.min_learning_rate = 1e-5
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        
        # ä½¿ç”¨ä½™å¼¦é€€ç«å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=50,  # åˆå§‹å‘¨æœŸ
            T_mult=2,  # å‘¨æœŸå€å¢å› å­
            eta_min=self.min_learning_rate
        )
        
        # ä¿®æ”¹æ¢ç´¢åƒæ•¸
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.2
        self.gamma = 0.95
        
        # åˆ†é›¢æ­£é¢å’Œè² é¢ç¶“é©—
        self.positive_memory = deque(maxlen=5000)
        self.negative_memory = deque(maxlen=5000)
        
        self.batch_size = 64
        self.target_update = 20
        self.env = SolitaireEnv()
        
        # ç”¨æ–¼è¿½è¹¤ä¸Šä¸€æ¬¡çš„æå¤±
        self.last_loss = None

    def calculate_priority(self, reward, done, action):
        """è¨ˆç®—ç¶“é©—çš„å„ªå…ˆç´š
        
        Args:
            reward (float): ç²å¾—çš„çå‹µ
            done (bool): æ˜¯å¦å®ŒæˆéŠæˆ²
            action (tuple): åŸ·è¡Œçš„å‹•ä½œ
            
        Returns:
            float: ç¶“é©—çš„å„ªå…ˆç´šå€¼
        """
        # åŸºç¤å„ªå…ˆç´šç‚ºçå‹µçš„çµ•å°å€¼
        priority = abs(reward) + 1.0
        
        # ç‰¹æ®Šæƒ…æ³çš„å„ªå…ˆç´šæå‡
        if done and reward > 0:  # æˆåŠŸå®ŒæˆéŠæˆ²
            priority *= 2.0
        
        if action[0] == 'tableau':
            if action[3] == 'foundation':  # ç§»åˆ°foundationçš„å‹•ä½œ
                priority *= 1.5
            elif len(action) > 2:  # ç§»å‹•å¤šå¼µç‰Œ
                priority *= 1.2
        
        elif action[0] == 'waste':
            if action[1] == 'foundation':  # wasteåˆ°foundation
                priority *= 1.5
            elif action[1] == 'tableau':   # wasteåˆ°tableau
                priority *= 1.2
        
        # æ ¹æ“šçå‹µçš„æ­£è² èª¿æ•´å„ªå…ˆç´š
        if reward > 0:
            priority *= 1.2  # æé«˜æ­£é¢ç¶“é©—çš„å„ªå…ˆç´š
        elif reward < -0.5:
            priority *= 1.1  # ç¨å¾®æé«˜å¤§çš„è² é¢ç¶“é©—çš„å„ªå…ˆç´š
        
        return priority

    def remember(self, state, action, reward, next_state, done):
        """å­˜å„²ç¶“é©—åˆ°è¨˜æ†¶ä¸­"""
        # ç¢ºä¿ç‹€æ…‹æ˜¯æ­£ç¢ºçš„æ ¼å¼
        if isinstance(state, tuple):
            state = state[0]  # å¦‚æœæ˜¯å…ƒçµ„ï¼Œå–ç¬¬ä¸€å€‹å…ƒç´ ï¼ˆç‹€æ…‹å‘é‡ï¼‰
        if isinstance(next_state, tuple):
            next_state = next_state[0]
            
        experience = (state, action, reward, next_state, done)
        priority = self.calculate_priority(reward, done, action)
        
        if reward > 0:
            self.positive_memory.append((experience, priority))
        else:
            self.negative_memory.append((experience, priority))

    def act(self, state):
        """é¸æ“‡å‹•ä½œ"""
        valid_moves = self.env.get_valid_moves()
        if not valid_moves:
            return None
        
        if random.random() <= self.epsilon:
            return random.choice(valid_moves)
        
        # ç¢ºä¿ç‹€æ…‹æ˜¯æ­£ç¢ºçš„æ ¼å¼
        if isinstance(state, tuple):
            state = state[0]
            
        state = torch.FloatTensor(state).unsqueeze(0)
        self.model.eval()
        with torch.no_grad():
            act_values = self.model(state)
        self.model.train()
        
        # å°‡Qå€¼æ˜ å°„åˆ°æœ‰æ•ˆç§»å‹•
        valid_move_values = []
        for move in valid_moves:
            move_idx = self._encode_action(move)
            if move_idx >= self.action_size:
                print(f"Warning: action index {move_idx} exceeds action size {self.action_size}")
                continue
            valid_move_values.append((move, act_values[0][move_idx].item()))
        
        if not valid_move_values:
            return random.choice(valid_moves)
        
        return max(valid_move_values, key=lambda x: x[1])[0]
    
    def _encode_action(self, action):
        """å°‡å‹•ä½œç·¨ç¢¼ç‚ºæ•´æ•¸ç´¢å¼•ï¼Œç¢ºä¿ç¯„åœåœ¨ 0-51 ä¹‹é–“"""
        if action[0] == 'stock':
            return 0
        elif action[0] == 'reset':
            return 1
        elif action[0] == 'waste':
            if action[1] == 'tableau':
                return 2 + min(action[2], 6)  # 2-8 (7å€‹tableauä½ç½®)
            else:  # foundation
                return 9 + min(action[2], 3)  # 9-12 (4å€‹foundationä½ç½®)
        elif action[0] == 'tableau':
            source_pile = min(action[1], 6)  # 0-6
            card_index = min(action[2], 12)   # 0-12
            
            if action[3] == 'tableau':
                # 13-38: tableauåˆ°tableauçš„ç§»å‹•
                dest_index = min(action[4], 6)
                return min(13 + (source_pile * 4 + dest_index), 51)
            else:  # foundation
                # 39-51: tableauåˆ°foundationçš„ç§»å‹•
                dest_index = min(action[4], 3)
                return min(39 + (source_pile * 2 + dest_index), 51)
        
        return 0  # é»˜èªæƒ…æ³
    
    def replay(self, batch_size):
        """å¾ç¶“é©—è¨˜æ†¶ä¸­æ¡æ¨£ä¸¦å­¸ç¿’"""
        # ç¢ºä¿æœ‰è¶³å¤ çš„ç¶“é©—å¯ä»¥æ¡æ¨£
        total_experiences = len(self.positive_memory) + len(self.negative_memory)
        if total_experiences < batch_size:
            return 0.0
        
        # æ ¹æ“šå¯ç”¨ç¶“é©—å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°
        pos_size = min(batch_size // 2, len(self.positive_memory))
        neg_size = min(batch_size - pos_size, len(self.negative_memory))
        actual_batch_size = pos_size + neg_size
        
        if actual_batch_size == 0:
            return 0.0
        
        minibatch = []
        
        # æ¡æ¨£æ­£é¢ç¶“é©—
        if pos_size > 0:
            pos_experiences = [x[0] for x in self.positive_memory]
            pos_priorities = np.array([x[1] for x in self.positive_memory])
            pos_probs = pos_priorities / pos_priorities.sum()
            pos_indices = np.random.choice(len(self.positive_memory), pos_size, p=pos_probs)
            pos_batch = [self.positive_memory[i][0] for i in pos_indices]
            minibatch.extend(pos_batch)
        
        # æ¡æ¨£è² é¢ç¶“é©—
        if neg_size > 0:
            neg_experiences = [x[0] for x in self.negative_memory]
            neg_priorities = np.array([x[1] for x in self.negative_memory])
            neg_probs = neg_priorities / neg_priorities.sum()
            neg_indices = np.random.choice(len(self.negative_memory), neg_size, p=neg_probs)
            neg_batch = [self.negative_memory[i][0] for i in neg_indices]
            minibatch.extend(neg_batch)
        
        if not minibatch:  # å¦‚æœæ²’æœ‰è¶³å¤ çš„æ¨£æœ¬
            return 0.0
            
        # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
        states = torch.FloatTensor(np.vstack([m[0] for m in minibatch]))
        actions = torch.LongTensor([self._encode_action(m[1]) for m in minibatch])
        rewards = torch.FloatTensor([m[2] for m in minibatch])
        next_states = torch.FloatTensor(np.vstack([m[3] for m in minibatch]))
        dones = torch.FloatTensor([m[4] for m in minibatch])
        
        # Double DQN with target network
        with torch.no_grad():
            next_actions = self.model(next_states).max(1)[1]
            next_q_values = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()
        
        # è¨ˆç®—ç›®æ¨™Qå€¼ä¸¦å¢åŠ çå‹µç¸®æ”¾
        target_q_values = rewards * 0.1 + (1 - dones) * self.gamma * next_q_values
        
        # è¨ˆç®—ç•¶å‰Qå€¼
        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze()
        
        # ä½¿ç”¨ Huber Loss
        loss = F.smooth_l1_loss(current_q_values, target_q_values)
        
        # æ›´æ–°æ¨¡å‹
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        self.scheduler.step()  # æ›´æ–°å­¸ç¿’ç‡
        
        return loss.item()
    
    def train(self, episodes, checkpoint_interval=None, checkpoint_dir=None):
        # ç¢ºä¿æª¢æŸ¥é»ç›®éŒ„å­˜åœ¨
        if checkpoint_dir:
            os.makedirs(checkpoint_dir, exist_ok=True)

        # åœ¨æ–¹æ³•é–‹å§‹è™•æ·»åŠ æ—¥èªŒè¨­ç½®
        log_file = os.path.join(checkpoint_dir, "training_log.txt") if checkpoint_dir else "training_log.txt"

        # ç¢ºä¿æ—¥èªŒæ–‡ä»¶çš„ç›®éŒ„å­˜åœ¨
        log_dir = os.path.dirname(log_file)
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)
        
        # å‰µå»ºä¸€å€‹æ—¥èªŒè¨˜éŒ„å™¨,åŒæ™‚è¼¸å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        class TeeLogger:
            def __init__(self, filename):
                self.terminal = sys.stdout
                self.log = open(filename, 'w', encoding='utf-8')
            
            def write(self, message):
                self.terminal.write(message)
                self.log.write(message)
                self.log.flush()
                
            def flush(self):
                self.terminal.flush()
                self.log.flush()
        
        # ä¿å­˜åŸå§‹çš„æ¨™æº–è¼¸å‡º
        original_stdout = sys.stdout
        sys.stdout = TeeLogger(log_file)
        
        try:
            # è¨˜éŒ„è¨“ç·´é–‹å§‹æ™‚é–“å’ŒåŸºæœ¬ä¿¡æ¯
            start_time = datetime.datetime.now()
            print(f"Training started at: {start_time}")
            print(f"Training configuration:")
            print(f"Episodes: {episodes}")
            print(f"Checkpoint interval: {checkpoint_interval}")
            print(f"Checkpoint directory: {checkpoint_dir}")
            print(f"Initial epsilon: {self.epsilon}")
            print(f"Learning rate: {self.learning_rate}")
            print("=" * 50)             
    
            best_reward = float('-inf')
            episode_rewards = []
            best_moves = 0
            no_improvement_count = 0
            prev_avg_reward = float('-inf')
            
            # å‰µå»ºæª¢æŸ¥é»ç›®éŒ„
            if checkpoint_interval and checkpoint_dir:
                os.makedirs(checkpoint_dir, exist_ok=True)

            # æ·»åŠ éŠæˆ²è¨˜éŒ„åŠŸèƒ½
            record_episode = 0
            print(f"Will record episode {record_episode}")

            for episode in range(episodes):
                # é‡ç½®ç’°å¢ƒä¸¦ç²å–åˆå§‹ç‹€æ…‹
                state = self.env.reset(self.custom_deck)
                total_reward = 0
                moves = 0
                episode_loss = 0.0
                loss_count = 0
                last_progress = 0
                
                # ç•¶å‰å›åˆçš„è¨˜éŒ„åˆå§‹åŒ–
                current_episode_record = []
                is_recording = (episode == record_episode)
                
                if is_recording:
                    print("\nStarting recorded episode...")
                    self.visualize_game(state)  # é¡¯ç¤ºåˆå§‹ç‹€æ…‹
                
                # æ›´æ–°ç›®æ¨™ç¶²çµ¡
                if episode % self.target_update == 0:
                    self.target_model.load_state_dict(self.model.state_dict())
                    print(f"Target network updated at episode {episode}")
                
                # å‹•æ…‹èª¿æ•´æ¢ç´¢ç‡
                if len(episode_rewards) > 100:
                    current_avg = np.mean(episode_rewards[-100:])
                    if episode > 100:
                        if current_avg < prev_avg_reward * 0.95:
                            self.epsilon = min(self.epsilon * 1.1, 0.9)
                            #print(f"Increasing exploration: epsilon = {self.epsilon:.3f}")
                        else:
                            self.epsilon = max(self.epsilon * 0.995, self.epsilon_min)
                    prev_avg_reward = current_avg

                while True:
                    action = self.act(state)
                    if action is None:  # æ²’æœ‰æœ‰æ•ˆç§»å‹•
                        break
                        
                    next_state, reward, done = self.env.step(action)
                    
                    # è¨˜éŒ„éŠæˆ²éç¨‹
                    if is_recording:
                        step_record = {
                            'move_number': moves + 1,
                            'action': action,
                            'reward': reward,
                            'foundation_cards': sum(len(pile) for pile in self.env.foundation),
                            'face_up_cards': sum(1 for pile in self.env.tableau for card in pile if card.is_face_up)
                        }
                        current_episode_record.append(step_record)
                        
                        # è¼¸å‡ºç•¶å‰å‹•ä½œå’Œç‹€æ…‹
                        print(f"\nMove {moves + 1}:")
                        print(f"Action: {self._format_action(action)}")
                        print(f"Reward: {reward:.2f}")
                        self.visualize_game(next_state)
                    
                    total_reward += reward
                    moves += 1
                    
                    # è¨ˆç®—éŠæˆ²é€²å±•
                    foundation_cards = sum(len(pile) for pile in self.env.foundation)
                    if foundation_cards > last_progress:
                        last_progress = foundation_cards
                        moves_limit = 500  # é‡ç½®ç§»å‹•é™åˆ¶
                    
                    # å­˜å„²ç¶“é©—
                    self.remember(state, action, reward, next_state, done)
                    
                    # æª¢æŸ¥ç¸½ç¶“é©—æ•¸é‡ä¸¦é€²è¡Œè¨“ç·´
                    total_memories = len(self.positive_memory) + len(self.negative_memory)
                    if total_memories >= self.batch_size:
                        loss = self.replay(self.batch_size)
                        episode_loss += loss
                        loss_count += 1
                        self.last_loss = loss  # æ›´æ–°æœ€å¾Œçš„æå¤±å€¼
                    
                    state = next_state
                    
                    if done:
                        if is_recording:
                            print("\nGame completed!")
                        break
                    
                    # å‹•æ…‹èª¿æ•´ç§»å‹•é™åˆ¶
                    base_moves = 300
                    extra_moves = foundation_cards * 20
                    tableau_cards = sum(1 for pile in self.env.tableau for card in pile if card.is_face_up)
                    extra_moves += tableau_cards * 10
                    current_limit = min(800, base_moves + extra_moves)
                    
                    if moves >= current_limit and foundation_cards == last_progress:
                        break
                
                # ä¿å­˜å›åˆè¨˜éŒ„
                if is_recording and checkpoint_dir:
                    record_path = os.path.join(checkpoint_dir, f"game_record_episode_{episode}.txt")
                    self._save_game_record(current_episode_record, record_path, total_reward, moves)
                    print(f"\nGame record saved to {record_path}")
                
                # episode çµæŸæ™‚æ›´æ–°å­¸ç¿’ç‡
                if loss_count > 0:
                    avg_loss = episode_loss / loss_count
                    self.scheduler.step()
                
                # è¨˜éŒ„ä¸¦è¼¸å‡ºè¨“ç·´é€²åº¦
                episode_rewards.append(total_reward)
                avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
                
                # æ›´æ–°æœ€ä½³è¨˜éŒ„
                if total_reward > best_reward:
                    best_reward = total_reward
                    best_moves = moves
                    no_improvement_count = 0
                else:
                    no_improvement_count += 1
                
                # æ¯10å±€è¼¸å‡ºä¸€æ¬¡è¨“ç·´ä¿¡æ¯
                if (episode + 1) % 10 == 0:
                    print(f"\nEpisode {episode + 1} Summary:")
                    print(f"Total Reward: {total_reward:.2f}")
                    print(f"Average Reward (last 100): {avg_reward:.2f}")
                    print(f"Best Reward: {best_reward:.2f}")
                    print(f"Moves Made: {moves}")
                    print(f"Foundation Cards: {foundation_cards}")
                    print(f"Face Up Cards: {tableau_cards}")
                    print(f"Current Epsilon: {self.epsilon:.3f}")
                    print(f"Current Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}")
                    print(f"Positive Memory Size: {len(self.positive_memory)}")
                    print(f"Negative Memory Size: {len(self.negative_memory)}")
                    if self.last_loss is not None:
                        print(f"Latest Loss: {self.last_loss:.4f}")
                    print("-" * 50)
                
                # ä¿å­˜æª¢æŸ¥é»
                if checkpoint_interval and checkpoint_dir and (episode + 1) % checkpoint_interval == 0:
                    checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_episode_{episode+1}.pt")
                    self.save_model(checkpoint_path)
                    print(f"\nCheckpoint saved at episode {episode + 1}")
                    print(f"Path: {checkpoint_path}")
                    print("-" * 50)
                
                # æå‰çµæŸæ¢ä»¶
                if len(episode_rewards) >= 100 and avg_reward > 95 and no_improvement_count > 200:
                    print(f"Training completed early at episode {episode + 1}")
                    print(f"Reason: Reached target performance")
                    break
                
                if no_improvement_count > 500:
                    print(f"Training completed early at episode {episode + 1}")
                    print(f"Reason: No improvement for too long")
                    break
                
                # å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡
                if no_improvement_count > 100:
                    self.learning_rate *= 0.5
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = self.learning_rate
                    #print(f"Reducing learning rate to {self.learning_rate}")
            
            end_time = datetime.datetime.now()
            duration = end_time - start_time
            
            print("\n=== Training Summary ===")
            print(f"Training completed at: {end_time}")
            print(f"Total training time: {duration}")
            print(f"\nFinal Results:")
            print(f"Best reward achieved: {best_reward}")
            print(f"Best moves in a single episode: {best_moves}")
            print(f"Final epsilon value: {self.epsilon:.3f}")
            print(f"Final learning rate: {self.optimizer.param_groups[0]['lr']:.6f}")
            print(f"Final positive memory size: {len(self.positive_memory)}")
            print(f"Final negative memory size: {len(self.negative_memory)}")
            print(f"\nStatistics:")
            print(f"Average reward: {np.mean(episode_rewards):.2f}")
            print(f"Standard deviation: {np.std(episode_rewards):.2f}")
            print(f"Maximum reward: {max(episode_rewards):.2f}")
            print(f"Minimum reward: {min(episode_rewards):.2f}")
            print(f"Median reward: {np.median(episode_rewards):.2f}")
            print(f"Last 100 episodes average: {np.mean(episode_rewards[-100:]):.2f}")
            print("="*50)
            
            return episode_rewards
    
        except Exception as e:
            print(f"\nTraining Error: {str(e)}")
            raise

        finally:
            sys.stdout = original_stdout
    
    def _format_action(self, action):
        """å°‡å‹•ä½œè½‰æ›ç‚ºå¯è®€çš„å­—ç¬¦ä¸²"""
        if action[0] == 'stock':
            return "Draw card from stock to waste"
        elif action[0] == 'reset':
            return "Reset waste to stock"
        elif action[0] == 'waste':
            dest = "tableau" if action[1] == 'tableau' else "foundation"
            return f"Move card from waste to {dest} pile {action[2]}"
        elif action[0] == 'tableau':
            source_pile = action[1]
            card_index = action[2]
            dest = "tableau" if action[3] == 'tableau' else "foundation"
            dest_pile = action[4]
            return f"Move card(s) from tableau {source_pile} (position {card_index}) to {dest} pile {dest_pile}"

    def _save_game_record(self, record, filepath, total_reward, total_moves):
        """ä¿å­˜éŠæˆ²è¨˜éŒ„åˆ°æ–‡ä»¶"""
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("=== Solitaire Game Record ===\n\n")
            f.write(f"Total Moves: {total_moves}\n")
            f.write(f"Total Reward: {total_reward:.2f}\n\n")
            
            for step in record:
                f.write(f"Move {step['move_number']}:\n")
                f.write(f"Action: {self._format_action(step['action'])}\n")
                f.write(f"Reward: {step['reward']:.2f}\n")
                f.write(f"Foundation Cards: {step['foundation_cards']}\n")
                f.write(f"Face Up Cards: {step['face_up_cards']}\n")
                f.write("-" * 50 + "\n")

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'target_model_state_dict': self.target_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'positive_memory': self.positive_memory,
            'negative_memory': self.negative_memory
        }
        torch.save(checkpoint, filepath)
        print(f"Model saved to {filepath}")

    def load_model(self, filepath):
        """å¾æ–‡ä»¶åŠ è¼‰æ¨¡å‹"""
        if not os.path.exists(filepath):
            print(f"No model file found at {filepath}")
            return False
            
        checkpoint = torch.load(filepath)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.positive_memory = checkpoint['positive_memory']
        self.negative_memory = checkpoint['negative_memory']
        print(f"Model loaded from {filepath}")
        return True

    def evaluate(self, num_episodes=100):
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        rewards = []
        moves_list = []
        win_count = 0
        foundation_cards_list = []
        
        original_epsilon = self.epsilon
        self.epsilon = 0.01  # åœ¨è©•ä¼°æ™‚ä½¿ç”¨è¼ƒå°çš„æ¢ç´¢ç‡
        
        for episode in range(episodes):
            # é‡ç½®ç’°å¢ƒä¸¦ç²å–åˆå§‹ç‹€æ…‹
            state, _ = self.env.reset(self.custom_deck)  # è§£åŒ…è¿”å›å€¼
            total_reward = 0
            moves = 0
            
            while True:
                action = self.act(state)
                if action is None:  # æ²’æœ‰æœ‰æ•ˆç§»å‹•
                    break
                    
                next_state, reward, done = self.env.step(action)
                
                # å­˜å„²ç¶“é©—
                self.remember(state, action, reward, next_state, done)
                
                # æ›´æ–°ç‹€æ…‹
                state = next_state
                
                if done:
                    win_count += 1
                    break
                    
                if moves >= 2000:  # è¨­ç½®æœ€å¤§ç§»å‹•æ¬¡æ•¸
                    break
            
            rewards.append(total_reward)
            moves_list.append(moves)
            foundation_cards = sum(len(pile) for pile in self.env.foundation)
            foundation_cards_list.append(foundation_cards)
            
            if (episode + 1) % 10 == 0:
                print(f"Evaluation Episode {episode + 1}")
                print(f"Reward: {total_reward:.2f}")
                print(f"Moves: {moves}")
                print(f"Foundation Cards: {foundation_cards}")
                print("------------------------")
        
        self.epsilon = original_epsilon
        self.model.train()
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        avg_reward = np.mean(rewards)
        avg_moves = np.mean(moves_list)
        avg_foundation = np.mean(foundation_cards_list)
        win_rate = win_count / num_episodes
        
        print("\nEvaluation Results:")
        print(f"Average Reward: {avg_reward:.2f}")
        print(f"Average Moves: {avg_moves:.2f}")
        print(f"Average Foundation Cards: {avg_foundation:.2f}")
        print(f"Win Rate: {win_rate:.2%}")
        
        return {
            'avg_reward': avg_reward,
            'avg_moves': avg_moves,
            'avg_foundation': avg_foundation,
            'win_rate': win_rate
        }

    def visualize_game(self, state):
        """å¯è¦–åŒ–ç•¶å‰éŠæˆ²ç‹€æ…‹"""
        tableau_str = "Tableau:\n"
        for i, pile in enumerate(self.env.tableau):
            tableau_str += f"{i}: "
            for card in pile:
                tableau_str += f"{str(card)} "
            tableau_str += "\n"
        
        foundation_str = "Foundation:\n"
        for i, pile in enumerate(self.env.foundation):
            foundation_str += f"{i}: "
            for card in pile:
                foundation_str += f"{str(card)} "
            foundation_str += "\n"
        
        waste_str = "Waste: "
        if self.env.waste:
            waste_str += str(self.env.waste[-1])
        
        stock_str = f"Stock: {len(self.env.stock)} cards"
        
        print("\n" + "="*50)
        print(foundation_str)
        print("-"*50)
        print(tableau_str)
        print("-"*50)
        print(waste_str)
        print(stock_str)
        print("="*50 + "\n")

if __name__ == "__main__":
    # é¸æ“‡æ˜¯å¦ä½¿ç”¨è‡ªå®šç¾©ç‰Œçµ„
    use_custom_deck = True  # æ”¹ç‚ºFalseå‰‡ä½¿ç”¨éš¨æ©Ÿç‰Œçµ„

    if use_custom_deck:
        # å®šç¾©è‡ªå®šç¾©ç‰Œçµ„
        custom_deck = [
            # ç¬¬ä¸€åˆ—çš„ç‰Œï¼ˆå¾ä¸Šåˆ°ä¸‹ï¼‰
            ('â™ ', 'Q'),
            # ç¬¬äºŒåˆ—çš„ç‰Œ
            ('â™£', '7'), ('â™£', '6'),
            # ç¬¬ä¸‰åˆ—çš„ç‰Œ
            ('â™£', 'J'), ('â™¥', 'K'), ('â™£', 'Q'),
            # ç¬¬å››åˆ—çš„ç‰Œ
            ('â™¦', '5'), ('â™¦', '6'), ('â™ ', '2'), ('â™£', '5'),
            # ç¬¬äº”åˆ—çš„ç‰Œ
            ('â™¥', '5'), ('â™£', '2'), ('â™ ', '5'), ('â™¥', '9'), ('â™ ', 'J'),
            # ç¬¬å…­åˆ—çš„ç‰Œ
            ('â™£', 'K'), ('â™£', '8'), ('â™¥', '6'), ('â™£', '10'), ('â™£', '9'), ('â™¥', '7'),
            # ç¬¬ä¸ƒåˆ—çš„ç‰Œ
            ('â™ ', '6'), ('â™¦', '8'), ('â™¦', '4'), ('â™ ', '3'), ('â™¦', '2'), ('â™£', '4'), ('â™¦', 'J'),
            # stockä¸­çš„ç‰Œï¼ˆå‰©é¤˜çš„ç‰Œï¼‰            
            ('â™ ', '9'), ('â™¦', '7'), ('â™¦', 'K'), ('â™¥', '2'), ('â™¦', 'Q'),
            ('â™£', '3'), ('â™¥', '8'), ('â™ ', '7'), ('â™ ', 'K'), ('â™¥', 'Q'),
            ('â™ ', '10'), ('â™¦', 'A'), ('â™ ', '8'), ('â™¥', '10'), ('â™ ', 'A'),
            ('â™¦', '9'), ('â™¥', '3'), ('â™¥', 'J'), ('â™¥', 'A'), ('â™ ', '4'),
            ('â™¦', '3'), ('â™¦', '10'), ('â™¥', '4'), ('â™£', 'A'),
        ]
    else:   
        # å‰µå»ºç’°å¢ƒå’ŒAIå¯¦ä¾‹
        env = SolitaireEnv()
        ai = SolitaireAI(custom_deck=custom_deck)
        
        # é‡ç½®ç’°å¢ƒï¼Œä½¿ç”¨è‡ªå®šç¾©ç‰Œçµ„
        initial_state, _ = env.reset(custom_deck)
        
        # é©—è­‰åˆå§‹ç‹€æ…‹
        print("\nInitial Game State:")
    
    # é¡¯ç¤ºåˆå§‹ç‹€æ…‹
    ai.visualize_game(initial_state)

    # è¨­ç½®ä¿å­˜ç›®éŒ„
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"training_runs/{timestamp}"
    checkpoint_dir = os.path.join(save_dir, "checkpoints")
    
    # å‰µå»ºå¿…è¦çš„ç›®éŒ„
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # å°‡ç‹€æ…‹è½‰æ›ç‚ºnumpyæ•¸çµ„ä»¥ä¾¿æŸ¥çœ‹å½¢ç‹€
    state_array = np.array(initial_state)
    print("Initial state shape:", state_array.shape)
    print("Initial state dtype:", state_array.dtype)
    print("Expected state size:", ai.state_size)
    print("Actual state size:", len(state_array))
    
    # æª¢æŸ¥ç‹€æ…‹å‘é‡çš„çµ„æˆ
    print("\nState vector composition:")
    print(f"Tableau section (0-272): {state_array[:273].shape}")
    print(f"Foundation section (273-284): {state_array[273:285].shape}")
    print(f"Waste section (285-287): {state_array[285:288].shape}")
    print(f"Stock section (288): {state_array[288]}")
    print(f"Hidden cards section (289-444): {state_array[289:].shape}")
    
    assert len(state_array) == ai.state_size, f"State size mismatch: got {len(state_array)}, expected {ai.state_size}"
    
    # é–‹å§‹è¨“ç·´
    try:
        rewards = ai.train(1000, checkpoint_interval=50, checkpoint_dir=checkpoint_dir)
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        ai.save_model(os.path.join(save_dir, "final_model.pt"))
        
        '''# é€²è¡Œæœ€çµ‚è©•ä¼°
        print("\nFinal Evaluation:")
        final_eval = ai.evaluate(num_episodes=100)
        
        # ä¿å­˜è©•ä¼°çµæœ
        with open(f"{save_dir}/evaluation_results.txt", 'w') as f:
            f.write(f"Final Evaluation Results:\n")
            for key, value in final_eval.items():
                f.write(f"{key}: {value}\n")
        
        # ç¹ªè£½è¨“ç·´é€²åº¦åœ–
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # ç¹ªè£½çå‹µæ›²ç·š
        ax1.plot(rewards)
        ax1.set_title('Training Progress - Rewards')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Total Reward')
        ax1.grid(True)
        
        # ç¹ªè£½ç§»å‹•å¹³å‡çå‹µæ›²ç·š
        window_size = 100
        moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
        ax2.plot(range(window_size-1, len(rewards)), moving_avg)
        ax2.set_title(f'Moving Average Reward (Window Size: {window_size})')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Average Reward')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'training_progress.png'))
        plt.close()
        
        # è¨ˆç®—ä¸¦è¼¸å‡ºè©³ç´°çš„çµ±è¨ˆä¿¡æ¯
        print("\nTraining Statistics:")
        print(f"Total Episodes: {len(rewards)}")
        print(f"Average Reward: {np.mean(rewards):.2f}")
        print(f"Standard Deviation: {np.std(rewards):.2f}")
        print(f"Maximum Reward: {max(rewards):.2f}")
        print(f"Minimum Reward: {min(rewards):.2f}")
        print(f"Median Reward: {np.median(rewards):.2f}")
        print(f"Last 100 Episodes Average: {np.mean(rewards[-100:]):.2f}")'''
        
    except Exception as e:
        print(f"Training error: {e}")
        raise
