import numpy as np
import random
from collections import deque
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import os
import datetime

# æ’²å…‹ç‰Œçš„èŠ±è‰²å’Œæ•¸å­—
SUITS = ['â™ ', 'â™¥', 'â™¦', 'â™£']
RANKS = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']

class Card:
    def __init__(self, suit, rank):
        self.suit = suit
        self.rank = rank
        self.is_face_up = False
    
    def __str__(self):
        if self.is_face_up:
            return f"{self.suit}{self.rank}"
        return "ğŸ‚ "  # ç‰ŒèƒŒ

class SolitaireEnv:
    def __init__(self):
        self.hidden_cards = {}  # è¿½è¸ªè“‹ä½çš„ç‰Œçš„ä½ç½®
        self.history = []      # æ·»åŠ æ­·å²è¨˜éŒ„
        self.reset()
    
    def reset(self):
        # åˆå§‹åŒ–ä¸€å‰¯æ–°ç‰Œ
        self.deck = [Card(suit, rank) for suit in SUITS for rank in RANKS]
        random.shuffle(self.deck)
        
        # åˆå§‹åŒ–éŠæˆ²å€åŸŸ
        self.tableau = [[] for _ in range(7)]  # 7åˆ—ä¸»è¦éŠæˆ²å€
        self.foundation = [[] for _ in range(4)]  # 4å€‹åŸºç¤å †(Aé–‹å§‹å¾€ä¸Šæ”¾)
        self.stock = []  # å‰©é¤˜çš„ç‰Œå †
        self.waste = []  # ç¿»é–‹çš„ç‰Œå †

        # è¨˜éŒ„è“‹ä½ç‰Œçš„ä½ç½®å’Œä¿¡æ¯
        self.hidden_cards = {}
        
        # ç™¼ç‰Œ
        for i in range(7):
            for j in range(i, 7):
                card = self.deck.pop()
                if i == j:  # æœ€ä¸Šé¢çš„ç‰Œæœä¸Š
                    card.is_face_up = True
                else:
                    # è¨˜éŒ„è“‹ä½çš„ç‰Œ
                    self.hidden_cards[(j, len(self.tableau[j]))] = (card.suit, card.rank)
                self.tableau[j].append(card)
        
        # å‰©ä¸‹çš„ç‰Œæ”¾å…¥stock
        self.stock = self.deck
        
        # ä¿å­˜åˆå§‹ç‹€æ…‹
        self._save_state()
        
        return self._get_state()
    
    def _get_state(self):
        # è¨ˆç®—å›ºå®šé•·åº¦çš„ç‹€æ…‹å‘é‡
        # tableau: 7åˆ— * 13å¼µç‰Œ * 3ç‰¹å¾µ = 273
        # foundation: 4å † * 3ç‰¹å¾µ = 12
        # waste: 1å¼µ * 3ç‰¹å¾µ = 3
        # stock: 1ç‰¹å¾µ = 1
        # hidden cards: 7 * 13 * 2 = 182
        # ç¸½å…±: 273 + 12 + 3 + 1 + 182 = 471å€‹ç‰¹å¾µ
        
        state = np.zeros(471, dtype=np.float32)
        current_idx = 0
        
        # ç·¨ç¢¼tableau
        for pile_idx, pile in enumerate(self.tableau):
            for card_idx in range(13):  # æ¯åˆ—æœ€å¤š13å¼µç‰Œ
                base_idx = current_idx + card_idx * 3
                if card_idx < len(pile):
                    card = pile[card_idx]
                    if card.is_face_up:
                        state[base_idx] = 1
                        state[base_idx + 1] = SUITS.index(card.suit) / 3
                        state[base_idx + 2] = RANKS.index(card.rank) / 12
                    else:
                        state[base_idx] = 0
                        state[base_idx + 1] = 0
                        state[base_idx + 2] = 0
                else:
                    state[base_idx] = -1
                    state[base_idx + 1] = -1
                    state[base_idx + 2] = -1
            current_idx += 39  # ç§»å‹•åˆ°ä¸‹ä¸€åˆ— (13 * 3 = 39)
        
        # ç·¨ç¢¼foundation
        for pile_idx, pile in enumerate(self.foundation):
            base_idx = current_idx + pile_idx * 3
            if pile:
                top_card = pile[-1]
                state[base_idx] = 1
                state[base_idx + 1] = SUITS.index(top_card.suit) / 3
                state[base_idx + 2] = RANKS.index(top_card.rank) / 12
            else:
                state[base_idx] = 0
                state[base_idx + 1] = 0
                state[base_idx + 2] = 0
        current_idx += 12  # ç§»å‹•åˆ°wasteå€åŸŸ (4 * 3 = 12)
        
        # ç·¨ç¢¼wasteé ‚ç‰Œ
        if self.waste:
            top_card = self.waste[-1]
            state[current_idx] = 1
            state[current_idx + 1] = SUITS.index(top_card.suit) / 3
            state[current_idx + 2] = RANKS.index(top_card.rank) / 12
        else:
            state[current_idx] = 0
            state[current_idx + 1] = 0
            state[current_idx + 2] = 0
        current_idx += 3
        
        # ç·¨ç¢¼stockæ˜¯å¦é‚„æœ‰ç‰Œ
        state[current_idx] = 1 if self.stock else 0
        current_idx += 1
        
        # ç·¨ç¢¼hidden cards
        for i in range(7):
            for j in range(13):
                base_idx = current_idx + (i * 13 + j) * 2
                pos = (i, j)
                if pos in self.hidden_cards:
                    suit, rank = self.hidden_cards[pos]
                    state[base_idx] = 1
                    # è¨ˆç®—ç‰Œçš„ä¼°è¨ˆåƒ¹å€¼
                    value = self._calculate_card_value(suit, rank)
                    state[base_idx + 1] = value
                else:
                    state[base_idx] = 0
                    state[base_idx + 1] = 0
        
        return state
    
    def _is_valid_move(self, card, destination, dest_pile):
        if destination == 'tableau':
            if not dest_pile:  # ç©ºåˆ—åªèƒ½æ”¾K
                return card.rank == 'K'
            top_card = dest_pile[-1]
            # æª¢æŸ¥é¡è‰²æ˜¯å¦ç›¸åä¸”æ•¸å­—æ˜¯å¦æŒ‰é †åº
            is_red = card.suit in ['â™¥', 'â™¦']
            top_is_red = top_card.suit in ['â™¥', 'â™¦']
            if is_red == top_is_red:
                return False
            return RANKS.index(card.rank) == RANKS.index(top_card.rank) - 1
            
        elif destination == 'foundation':
            if not dest_pile:  # ç©ºåŸºç¤å †åªèƒ½æ”¾A
                return card.rank == 'A'
            top_card = dest_pile[-1]
            # æª¢æŸ¥æ˜¯å¦åŒèŠ±è‰²ä¸”æ•¸å­—é€£çºŒ
            return (card.suit == top_card.suit and 
                   RANKS.index(card.rank) == RANKS.index(top_card.rank) + 1)
    
    def get_valid_moves(self):
        valid_moves = []
        
        # å¾wasteåˆ°tableauæˆ–foundationçš„ç§»å‹•
        if self.waste:
            card = self.waste[-1]
            # æª¢æŸ¥åˆ°tableauçš„ç§»å‹•
            for i in range(7):
                if self._is_valid_move(card, 'tableau', self.tableau[i]):
                    valid_moves.append(('waste', 'tableau', i))
            # æª¢æŸ¥åˆ°foundationçš„ç§»å‹•
            for i in range(4):
                if self._is_valid_move(card, 'foundation', self.foundation[i]):
                    valid_moves.append(('waste', 'foundation', i))
        
        # å¾tableauåˆ°tableauæˆ–foundationçš„ç§»å‹•
        for i in range(7):
            if not self.tableau[i]:
                continue
            for j in range(len(self.tableau[i])):
                if not self.tableau[i][j].is_face_up:
                    continue
                card = self.tableau[i][j]
                # æª¢æŸ¥åˆ°å…¶ä»–tableauåˆ—çš„ç§»å‹•
                for k in range(7):
                    if k != i and self._is_valid_move(card, 'tableau', self.tableau[k]):
                        valid_moves.append(('tableau', i, j, 'tableau', k))
                # æª¢æŸ¥åˆ°foundationçš„ç§»å‹•
                if j == len(self.tableau[i]) - 1:  # åªèƒ½ç§»å‹•é ‚ç‰Œåˆ°foundation
                    for k in range(4):
                        if self._is_valid_move(card, 'foundation', self.foundation[k]):
                            valid_moves.append(('tableau', i, j, 'foundation', k))
        
        # ç¿»stockç‰Œçš„æ“ä½œ
        if self.stock:
            valid_moves.append(('stock', 'waste'))
        elif self.waste:  # stockç©ºæ™‚å¯ä»¥å°‡wasteé‡ç½®
            valid_moves.append(('reset',))
            
        return valid_moves
    
    def _calculate_card_value(self, suit, rank):
        """è¨ˆç®—ç‰Œçš„ä¼°è¨ˆåƒ¹å€¼"""
        # åŸºç¤åƒ¹å€¼
        base_value = RANKS.index(rank) / 12.0
        
        # æ ¹æ“šç•¶å‰foundationçš„æƒ…æ³èª¿æ•´åƒ¹å€¼
        for pile in self.foundation:
            if pile and pile[-1].suit == suit:
                if RANKS.index(rank) == RANKS.index(pile[-1].rank) + 1:
                    base_value *= 1.5  # å¦‚æœæ˜¯ä¸‹ä¸€å¼µéœ€è¦çš„ç‰Œ,æé«˜åƒ¹å€¼
                    break
        
        # æ ¹æ“štableauçš„æƒ…æ³èª¿æ•´
        for pile in self.tableau:
            if pile and pile[-1].is_face_up:
                top_card = pile[-1]
                if (RANKS.index(rank) == RANKS.index(top_card.rank) - 1 and
                    ((suit in ['â™¥', 'â™¦']) != (top_card.suit in ['â™¥', 'â™¦']))):
                    base_value *= 1.2  # å¦‚æœå¯ä»¥æ”¾åœ¨tableauä¸Š,ç¨å¾®æé«˜åƒ¹å€¼
                    break
        
        # æ ¹æ“šranké¡å¤–èª¿æ•´
        if rank == 'A':
            base_value *= 1.3  # Açš„åƒ¹å€¼ç•¥é«˜ï¼Œå› ç‚ºæ˜¯foundationçš„èµ·å§‹ç‰Œ
        elif rank == 'K':
            base_value *= 1.2  # Kçš„åƒ¹å€¼ä¹Ÿç•¥é«˜ï¼Œå› ç‚ºå¯ä»¥é–‹æ–°åˆ—
            
        # å¦‚æœæ˜¯ç´…å¿ƒæˆ–æ–¹å¡Šï¼Œç¨å¾®é™ä½åƒ¹å€¼ï¼ˆå› ç‚ºéœ€è¦é»‘æ¡ƒæˆ–æ¢…èŠ±ä½œç‚ºåŸºç¤ï¼‰
        if suit in ['â™¥', 'â™¦']:
            base_value *= 0.9
            
        return base_value

    def _save_state(self):
        """ä¿å­˜ç•¶å‰éŠæˆ²ç‹€æ…‹"""
        state = {
            'tableau': [[str(card) for card in pile] for pile in self.tableau],
            'foundation': [[str(card) for card in pile] for pile in self.foundation],
            'stock': [str(card) for card in self.stock],
            'waste': [str(card) for card in self.waste],
            'hidden_cards': {str(k): v for k, v in self.hidden_cards.items()}
        }
        self.history.append(state)

    def _update_hidden_cards(self):
        """æ›´æ–°è“‹ä½çš„ç‰Œçš„ä¿¡æ¯"""
        new_hidden_cards = {}
        for i, pile in enumerate(self.tableau):
            for j, card in enumerate(pile):
                if not card.is_face_up:
                    new_hidden_cards[(i, j)] = (card.suit, card.rank)
        self.hidden_cards = new_hidden_cards

    def step(self, action):
        # ä¿å­˜ç•¶å‰ç‹€æ…‹ç”¨æ–¼è¨ˆç®—reward
        prev_valid_moves = len(self.get_valid_moves())
        prev_foundation_cards = sum(len(pile) for pile in self.foundation)
        prev_face_up_cards = sum(1 for pile in self.tableau for card in pile if card.is_face_up)
        
        # ä¿å­˜ç•¶å‰ç‹€æ…‹
        self._save_state()
        
        reward = 0
        done = False
        
        if action[0] == 'stock':
            # å¾stockç¿»ç‰Œåˆ°waste
            card = self.stock.pop()
            card.is_face_up = True
            self.waste.append(card)
            reward = -0.1
            
        elif action[0] == 'reset':
            # é‡ç½®wasteåˆ°stock
            self.stock = list(reversed(self.waste))
            self.waste = []
            for card in self.stock:
                card.is_face_up = False
            reward = -0.2
            
        elif action[0] == 'waste':
            card = self.waste.pop()
            if action[1] == 'tableau':
                self.tableau[action[2]].append(card)
                reward = 1.0
            else:  # foundation
                self.foundation[action[2]].append(card)
                reward = 2.0
                
        elif action[0] == 'tableau':
            source_pile = self.tableau[action[1]]
            cards_to_move = source_pile[action[2]:]
            if action[3] == 'tableau':
                self.tableau[action[4]].extend(cards_to_move)
                del source_pile[action[2]:]
                reward = 0.5
            else:  # foundation
                self.foundation[action[4]].append(cards_to_move[0])
                del source_pile[-1]
                reward = 2.0
            
            # ç¿»é–‹ç§»å‹•å¾Œéœ²å‡ºçš„ç‰Œ
            if source_pile and not source_pile[-1].is_face_up:
                source_pile[-1].is_face_up = True
                # æ›´æ–°hidden_cards
                if (action[1], len(source_pile)-1) in self.hidden_cards:
                    del self.hidden_cards[(action[1], len(source_pile)-1)]
                reward += 1.0
        
        # æ›´æ–°hidden_cards
        self._update_hidden_cards()
        
        # è¨ˆç®—é€²å±•çå‹µ
        current_valid_moves = len(self.get_valid_moves())
        moves_diff = current_valid_moves - prev_valid_moves
        
        if moves_diff > 0:
            reward += moves_diff * 0.2
        
        current_foundation_cards = sum(len(pile) for pile in self.foundation)
        current_face_up_cards = sum(1 for pile in self.tableau for card in pile if card.is_face_up)
        
        # ç‰¹æ®Šçå‹µï¼šå®Œæˆä¸€å€‹èŠ±è‰²çš„åºåˆ—
        for foundation in self.foundation:
            if len(foundation) > 0:
                if foundation[-1].rank == 'K':
                    reward += 5.0
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é€£çºŒçš„foundationç‰Œ
        for i in range(4):
            if len(self.foundation[i]) >= 2:
                reward += len(self.foundation[i]) * 0.5

        # é¡å¤–çš„é€²å±•çå‹µ
        foundation_progress = current_foundation_cards - prev_foundation_cards
        face_up_progress = current_face_up_cards - prev_face_up_cards
        
        reward += foundation_progress * 2.0
        reward += face_up_progress * 0.5
        
        # æ ¹æ“šhidden_cardsçš„ä¿¡æ¯èª¿æ•´çå‹µ
        for pos, (suit, rank) in self.hidden_cards.items():
            value = self._calculate_card_value(suit, rank)
            if value > 0.7:  # å¦‚æœæ˜¯é«˜åƒ¹å€¼çš„ç‰Œé‚„è¢«è“‹è‘—
                reward -= 0.1  # è¼•å¾®æ‡²ç½°
        
        # æª¢æŸ¥æ˜¯å¦ç²å‹
        if all(len(pile) == 13 for pile in self.foundation):
            reward = 50
            done = True
            
        return self._get_state(), reward, done
    
        # å¢åŠ é€£çºŒæˆåŠŸçš„çå‹µ
        if foundation_progress > 0:
            reward += foundation_progress * 3.0  # å¢åŠ åŸºç¤çå‹µ
            
        # å¢åŠ å®Œæˆç‰¹å®šç›®æ¨™çš„çå‹µ
        if current_foundation_cards >= 13:  # å®Œæˆä¸€çµ„
            reward += 10.0
            
        # æ‡²ç½°ç„¡æ•ˆçš„å¾ªç’°
        if moves > 100 and foundation_progress == 0:
            reward -= 0.5 * (moves - 100) / 100

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        
        # ä¸»å¹¹ç¶²çµ¡
        self.fc1 = nn.Linear(input_size, 1024)
        self.ln1 = nn.LayerNorm(1024)
        self.fc2 = nn.Linear(1024, 512)
        self.ln2 = nn.LayerNorm(512)
        self.fc3 = nn.Linear(512, 256)
        self.ln3 = nn.LayerNorm(256)
        
        # æ®˜å·®é€£æ¥
        self.residual1 = nn.Linear(input_size, 512)
        self.residual2 = nn.Linear(512, 256)
        
        # æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.attention = nn.MultiheadAttention(256, num_heads=4, batch_first=True)
        
        # è¼¸å‡ºå±¤
        self.fc4 = nn.Linear(256, 128)
        self.ln4 = nn.LayerNorm(128)
        self.fc5 = nn.Linear(128, output_size)
        
        # é™ä½ dropout ç‡ä½†å¢åŠ ä½ç½®
        self.dropout1 = nn.Dropout(0.1)
        self.dropout2 = nn.Dropout(0.1)
        self.dropout3 = nn.Dropout(0.1)
        
        # ä½¿ç”¨ GELU æ¿€æ´»å‡½æ•¸
        self.gelu = nn.GELU()
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # ä½¿ç”¨ Kaiming åˆå§‹åŒ–
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x, training=False):
        # ä¸»å¹¹ç¶²çµ¡å‰å‘å‚³æ’­
        identity = x
        
        x = self.fc1(x)
        x = self.ln1(x)
        x = self.gelu(x)
        if training:
            x = self.dropout1(x)
        
        x = self.fc2(x)
        x = self.ln2(x)
        x = self.gelu(x)
        
        # ç¬¬ä¸€å€‹æ®˜å·®é€£æ¥
        residual = self.residual1(identity)
        x = x + residual
        
        if training:
            x = self.dropout2(x)
        
        x = self.fc3(x)
        x = self.ln3(x)
        x = self.gelu(x)
        
        # ç¬¬äºŒå€‹æ®˜å·®é€£æ¥
        residual = self.residual2(residual)
        x = x + residual
        
        # æ³¨æ„åŠ›æ©Ÿåˆ¶
        # é‡å¡‘å¼µé‡ä»¥é©æ‡‰æ³¨æ„åŠ›å±¤
        batch_size = x.size(0)
        x = x.view(batch_size, 1, -1)  # [batch_size, 1, features]
        x, _ = self.attention(x, x, x)
        x = x.squeeze(1)  # [batch_size, features]
        
        if training:
            x = self.dropout3(x)
        
        # è¼¸å‡ºå±¤
        x = self.fc4(x)
        x = self.ln4(x)
        x = self.gelu(x)
        x = self.fc5(x)
        
        return x

    def get_attention_weights(self, x):
        """ç²å–æ³¨æ„åŠ›æ¬Šé‡ç”¨æ–¼å¯è¦–åŒ–"""
        batch_size = x.size(0)
        x = x.view(batch_size, 1, -1)
        _, weights = self.attention(x, x, x, need_weights=True)
        return weights

class SolitaireAI:
    def __init__(self):
        self.env = SolitaireEnv()
        self.state_size = 471  # æ›´æ–°ç‚ºæ–°çš„ç‹€æ…‹å‘é‡å¤§å°
        self.action_size = 100
        self.memory = deque(maxlen=10000)
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.9995
        self.learning_rate = 0.0003
        self.batch_size = 1024
        self.target_update = 20
        
        self.model = DQN(self.state_size, self.action_size)
        self.target_model = DQN(self.state_size, self.action_size)
        self.target_model.load_state_dict(self.model.state_dict())
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 
            mode='min', 
            factor=0.1,
            patience=100,
            min_lr=1e-5,
            verbose=True
        )
        self.criterion = nn.MSELoss()

    def remember(self, state, action, reward, next_state, done):
        # åŸºç¤å„ªå…ˆç´š
        priority = abs(reward) + 1.0
        # å¢åŠ å„ªå…ˆç´šè¨ˆç®—
        priority = abs(reward) + 1.0
        
        # æ ¹æ“šå‹•ä½œé¡å‹èª¿æ•´å„ªå…ˆç´š
        if action[0] == 'tableau' and action[3] == 'foundation':
            priority *= 1.3
        elif action[0] == 'waste' and action[1] == 'foundation':
            priority *= 1.2
        # æ ¹æ“šfoundationé€²å±•å¢åŠ å„ªå…ˆç´š
        if action[0] == 'tableau' and action[3] == 'foundation':
            priority *= 2.0  # å¢åŠ æ¬Šé‡

        # æ·»åŠ æ™‚é–“è¡°æ¸›å› å­
        if len(self.memory) > 0:
            oldest_priority = self.memory[0][5]
            priority *= 0.99  # è¼•å¾®çš„æ™‚é–“è¡°æ¸›
        # å¢åŠ æˆåŠŸåºåˆ—çš„å„ªå…ˆç´š
        if reward > 5:  # è¡¨ç¤ºæœ‰é‡è¦é€²å±•
            priority *= 1.5
        
        self.memory.append((state, action, reward, next_state, done, priority))
    
    def act(self, state):
        valid_moves = self.env.get_valid_moves()
        if not valid_moves:
            return None
        
        if random.random() <= self.epsilon:
            return random.choice(valid_moves)
        
        state = torch.FloatTensor(state).unsqueeze(0)
        self.model.eval()  # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
        with torch.no_grad():
            act_values = self.model(state, training=False)
        self.model.train()  # æ¢å¾©ç‚ºè¨“ç·´æ¨¡å¼
        
        # å°‡Qå€¼æ˜ å°„åˆ°æœ‰æ•ˆç§»å‹•
        valid_move_values = []
        for move in valid_moves:
            move_idx = self._encode_action(move)
            valid_move_values.append((move, act_values[0][move_idx].item()))
        return max(valid_move_values, key=lambda x: x[1])[0]
    
    def _encode_action(self, action):
        # å°‡å‹•ä½œç·¨ç¢¼ç‚ºæ•´æ•¸ç´¢å¼•
        if action[0] == 'stock':
            return 0
        elif action[0] == 'reset':
            return 1
        elif action[0] == 'waste':
            if action[1] == 'tableau':
                return 2 + action[2]
            else:  # foundation
                return 9 + action[2]
        elif action[0] == 'tableau':
            return 13 + action[1] * 7 + action[2]
        return 0
    
    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return 0.0
        
        # ä½¿ç”¨æº«å’Œçš„å„ªå…ˆç´šæ¡æ¨£
        priorities = np.array([m[5] for m in self.memory])
        probs = (priorities - priorities.min()) / (priorities.max() - priorities.min() + 1e-7)
        probs = probs / probs.sum()
        
        indices = np.random.choice(len(self.memory), batch_size, p=probs)
        minibatch = [self.memory[i] for i in indices]
        
        states = torch.FloatTensor(np.vstack([m[0] for m in minibatch]))
        actions = torch.LongTensor([self._encode_action(m[1]) for m in minibatch])
        rewards = torch.FloatTensor([m[2] for m in minibatch])
        next_states = torch.FloatTensor(np.vstack([m[3] for m in minibatch]))
        dones = torch.FloatTensor([m[4] for m in minibatch])

        # Double DQN with target network
        with torch.no_grad():
            next_actions = self.model(next_states).max(1)[1]
            next_q_values = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()
        
        # è¨ˆç®—ç›®æ¨™Qå€¼ä¸¦å¢åŠ çå‹µç¸®æ”¾
        target_q_values = rewards * 0.1 + (1 - dones) * self.gamma * next_q_values
        
        # è¨ˆç®—ç•¶å‰Qå€¼
        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze()
        
        # ä½¿ç”¨ Huber Loss
        loss = F.smooth_l1_loss(current_q_values, target_q_values)
        
        # æ›´æ–°æ¨¡å‹
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        # å‹•æ…‹å­¸ç¿’ç‡èª¿æ•´
        if hasattr(self, 'last_loss') and self.last_loss is not None:
            if loss.item() > self.last_loss * 1.2:  # æ›´å¯¬é¬†çš„é–¾å€¼
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] *= 0.95  # æ›´æº«å’Œçš„è¡°æ¸›
        
        self.last_loss = loss.item()
        
        return loss.item()
    
    def train(self, episodes, checkpoint_interval=None, checkpoint_dir=None):
        """è¨“ç·´AIç©æ¥é¾éŠæˆ²
        
        Args:
            episodes (int): è¦è¨“ç·´çš„å›åˆæ•¸
            checkpoint_interval (int, optional): ä¿å­˜æª¢æŸ¥é»çš„é–“éš”å›åˆæ•¸
            checkpoint_dir (str, optional): æª¢æŸ¥é»ä¿å­˜ç›®éŒ„
        """
        best_reward = float('-inf')
        episode_rewards = []
        best_moves = 0
        no_improvement_count = 0
        episode_loss = 0.0
        loss_count = 0
        
        # å‰µå»ºæª¢æŸ¥é»ç›®éŒ„
        if checkpoint_interval and checkpoint_dir:
            os.makedirs(checkpoint_dir, exist_ok=True)
        
        for episode in range(episodes):
            # æ›´æ–°ç›®æ¨™ç¶²çµ¡
            if episode % self.target_update == 0:
                self.target_model.load_state_dict(self.model.state_dict())
                print(f"Target network updated at episode {episode}")
                
            # åœ¨æ¯å€‹episodeé–‹å§‹æ™‚æ›´æ–°epsilon
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
            
            # å¦‚æœæœ‰è¶³å¤ çš„æ­·å²è¨˜éŒ„ï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦å¢åŠ æ¢ç´¢
            if len(episode_rewards) > 100:
                recent_avg = np.mean(episode_rewards[-100:])
                if len(episode_rewards) > 200:
                    previous_avg = np.mean(episode_rewards[-200:-100])
                    if recent_avg < previous_avg * 0.95:
                        self.epsilon = min(self.epsilon * 1.05, 0.8)
                        print(f"Increasing exploration: epsilon = {self.epsilon:.3f}")
            
            state = self.env.reset()
            total_reward = 0
            moves = 0
            last_progress = 0
            episode_loss = 0
            loss_count = 0
            
            while True:
                action = self.act(state)
                if action is None:  # æ²’æœ‰æœ‰æ•ˆç§»å‹•
                    break
                    
                next_state, reward, done = self.env.step(action)
                total_reward += reward
                moves += 1
                
                # è¨ˆç®—éŠæˆ²é€²å±•
                foundation_cards = sum(len(pile) for pile in self.env.foundation)
                if foundation_cards > last_progress:
                    last_progress = foundation_cards
                    moves_limit = 500  # é‡ç½®ç§»å‹•é™åˆ¶
                
                # å­˜å„²ç¶“é©—
                self.remember(state, action, reward, next_state, done)
                
                # å¦‚æœæœ‰è¶³å¤ çš„ç¶“é©—ï¼Œé€²è¡Œæ‰¹é‡å­¸ç¿’
                if len(self.memory) >= self.batch_size:
                    loss = self.replay(self.batch_size)
                    episode_loss += loss
                    loss_count += 1
                
                state = next_state
                
                if done:
                    break
                
                # å‹•æ…‹èª¿æ•´ç§»å‹•é™åˆ¶
                base_moves = 300
                extra_moves = foundation_cards * 20
                tableau_cards = sum(1 for pile in self.env.tableau for card in pile if card.is_face_up)
                extra_moves += tableau_cards * 10
                current_limit = min(800, base_moves + extra_moves)
                
                if moves >= current_limit and foundation_cards == last_progress:
                    break
            
            # episode çµæŸæ™‚æ›´æ–°å­¸ç¿’ç‡
            if loss_count > 0:
                avg_loss = episode_loss / loss_count
                self.scheduler.step(avg_loss)
                
            # è¨˜éŒ„ä¸¦è¼¸å‡ºè¨“ç·´é€²åº¦
            episode_rewards.append(total_reward)
            avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
            
            # æ›´æ–°æœ€ä½³è¨˜éŒ„
            if total_reward > best_reward:
                best_reward = total_reward
                best_moves = moves
                no_improvement_count = 0
            else:
                no_improvement_count += 1
            
            # æ¯10å±€è¼¸å‡ºä¸€æ¬¡è¨“ç·´ä¿¡æ¯
            if (episode + 1) % 10 == 0:
                print(f"Episode: {episode + 1}")
                print(f"Total Reward: {total_reward}")
                print(f"Average Reward (last 100): {avg_reward:.2f}")
                print(f"Best Reward: {best_reward}")
                print(f"Best Moves: {best_moves}")
                print(f"Epsilon: {self.epsilon:.3f}")
                print(f"Memory Size: {len(self.memory)}")
                print(f"Moves: {moves}")
                print(f"Foundation Cards: {foundation_cards}")
                print(f"Tableau Face Up Cards: {sum(1 for pile in self.env.tableau for card in pile if card.is_face_up)}")
                print(f"Loss: {self.last_loss:.4f}")
                print("------------------------")
            
            # ä¿å­˜æª¢æŸ¥é»
            if checkpoint_interval and checkpoint_dir and (episode + 1) % checkpoint_interval == 0:
                checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_episode_{episode+1}.pt")
                self.save_model(checkpoint_path)
                print(f"Saved checkpoint to {checkpoint_path}")
            
            # æå‰çµæŸæ¢ä»¶
            if len(episode_rewards) >= 100 and avg_reward > 95 and no_improvement_count > 200:
                print(f"Training completed early at episode {episode + 1}")
                print(f"Reason: Reached target performance")
                break
            
            if no_improvement_count > 500:
                print(f"Training completed early at episode {episode + 1}")
                print(f"Reason: No improvement for too long")
                break
            # å¢åŠ æ—©æœŸåœæ­¢æ¢ä»¶
            patience = 200  # å¢åŠ è€å¿ƒå€¼
            min_delta = 0.01  # æœ€å°æ”¹é€²é–¾å€¼

            # å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡
            if no_improvement_count > 100:
                self.learning_rate *= 0.5
                print(f"Reducing learning rate to {self.learning_rate}")
        
        print("\nTraining finished.")
        print(f"Best reward achieved: {best_reward}")
        print(f"Best moves in a single episode: {best_moves}")
        print(f"Final epsilon value: {self.epsilon:.3f}")
        print(f"Final memory size: {len(self.memory)}")
        return episode_rewards
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'target_model_state_dict': self.target_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'memory': self.memory
        }
        torch.save(checkpoint, filepath)
        print(f"Model saved to {filepath}")

    def load_model(self, filepath):
        """å¾æ–‡ä»¶åŠ è¼‰æ¨¡å‹"""
        if not os.path.exists(filepath):
            print(f"No model file found at {filepath}")
            return False
            
        checkpoint = torch.load(filepath)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.memory = checkpoint['memory']
        print(f"Model loaded from {filepath}")
        return True

    def evaluate(self, num_episodes=100):
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        rewards = []
        moves_list = []
        win_count = 0
        foundation_cards_list = []
        
        original_epsilon = self.epsilon
        self.epsilon = 0.01  # åœ¨è©•ä¼°æ™‚ä½¿ç”¨è¼ƒå°çš„æ¢ç´¢ç‡
        
        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0
            moves = 0
            
            while True:
                action = self.act(state)
                if action is None:
                    break
                    
                next_state, reward, done = self.env.step(action)
                total_reward += reward
                moves += 1
                state = next_state
                
                if done:
                    win_count += 1
                    break
                    
                if moves >= 2000:  # è¨­ç½®æœ€å¤§ç§»å‹•æ¬¡æ•¸
                    break
            
            rewards.append(total_reward)
            moves_list.append(moves)
            foundation_cards = sum(len(pile) for pile in self.env.foundation)
            foundation_cards_list.append(foundation_cards)
            
            if (episode + 1) % 10 == 0:
                print(f"Evaluation Episode {episode + 1}")
                print(f"Reward: {total_reward:.2f}")
                print(f"Moves: {moves}")
                print(f"Foundation Cards: {foundation_cards}")
                print("------------------------")
        
        self.epsilon = original_epsilon
        self.model.train()
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        avg_reward = np.mean(rewards)
        avg_moves = np.mean(moves_list)
        avg_foundation = np.mean(foundation_cards_list)
        win_rate = win_count / num_episodes
        
        print("\nEvaluation Results:")
        print(f"Average Reward: {avg_reward:.2f}")
        print(f"Average Moves: {avg_moves:.2f}")
        print(f"Average Foundation Cards: {avg_foundation:.2f}")
        print(f"Win Rate: {win_rate:.2%}")
        
        return {
            'avg_reward': avg_reward,
            'avg_moves': avg_moves,
            'avg_foundation': avg_foundation,
            'win_rate': win_rate
        }

    def visualize_game(self, state):
        """å¯è¦–åŒ–ç•¶å‰éŠæˆ²ç‹€æ…‹"""
        tableau_str = "Tableau:\n"
        for i, pile in enumerate(self.env.tableau):
            tableau_str += f"{i}: "
            for card in pile:
                tableau_str += f"{str(card)} "
            tableau_str += "\n"
        
        foundation_str = "Foundation:\n"
        for i, pile in enumerate(self.env.foundation):
            foundation_str += f"{i}: "
            for card in pile:
                foundation_str += f"{str(card)} "
            foundation_str += "\n"
        
        waste_str = "Waste: "
        if self.env.waste:
            waste_str += str(self.env.waste[-1])
        
        stock_str = f"Stock: {len(self.env.stock)} cards"
        
        print("\n" + "="*50)
        print(foundation_str)
        print("-"*50)
        print(tableau_str)
        print("-"*50)
        print(waste_str)
        print(stock_str)
        print("="*50 + "\n")

if __name__ == "__main__":
    ai = SolitaireAI()
    
    # è¨­ç½®ä¿å­˜ç›®éŒ„
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"training_runs/{timestamp}"
    os.makedirs(save_dir, exist_ok=True)

    # æ¸¬è©¦ç‹€æ…‹å‘é‡
    state = ai.env.reset()
    print("Initial state shape:", state.shape)
    print("Initial state dtype:", state.dtype)
    print("Expected state size:", ai.state_size)
    assert len(state) == ai.state_size, f"State size mismatch: got {len(state)}, expected {ai.state_size}"
    
    # é–‹å§‹è¨“ç·´
    try:
        rewards = ai.train(1000, checkpoint_interval=30, checkpoint_dir=f"{save_dir}/checkpoints")
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        ai.save_model(f"{save_dir}/final_model.pt")
        
        # é€²è¡Œæœ€çµ‚è©•ä¼°
        print("\nFinal Evaluation:")
        final_eval = ai.evaluate(num_episodes=100)
        
        # ä¿å­˜è©•ä¼°çµæœ
        with open(f"{save_dir}/evaluation_results.txt", 'w') as f:
            f.write(f"Final Evaluation Results:\n")
            for key, value in final_eval.items():
                f.write(f"{key}: {value}\n")
        
        # ç¹ªè£½è¨“ç·´é€²åº¦åœ–
        
        # å‰µå»ºå…©å€‹å­åœ–
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # ç¹ªè£½çå‹µæ›²ç·š
        ax1.plot(rewards)
        ax1.set_title('Training Progress - Rewards')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Total Reward')
        ax1.grid(True)
        
        # ç¹ªè£½ç§»å‹•å¹³å‡çå‹µæ›²ç·š
        window_size = 100
        moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
        ax2.plot(range(window_size-1, len(rewards)), moving_avg)
        ax2.set_title(f'Moving Average Reward (Window Size: {window_size})')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Average Reward')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_progress.png')
        plt.close()
        
        # è¨ˆç®—ä¸¦è¼¸å‡ºè©³ç´°çš„çµ±è¨ˆä¿¡æ¯
        print("\nTraining Statistics:")
        print(f"Total Episodes: {len(rewards)}")
        print(f"Average Reward: {np.mean(rewards):.2f}")
        print(f"Standard Deviation: {np.std(rewards):.2f}")
        print(f"Maximum Reward: {max(rewards):.2f}")
        print(f"Minimum Reward: {min(rewards):.2f}")
        print(f"Median Reward: {np.median(rewards):.2f}")
        print(f"Last 100 Episodes Average: {np.mean(rewards[-100:]):.2f}")
        
    except Exception as e:
        print(f"Training error: {e}")
        raise